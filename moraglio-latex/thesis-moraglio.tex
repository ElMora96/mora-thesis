% !TEX encoding = UTF-8 Unicode
% !TEX TS-program = pdflatex

%%%%%%% La riga soprastante serve per configurare gli editor
%%%%%%% TeXShop, TeXworks e TeXstudio per gestire questo file
%%%%%%% con la codifica UFF-8.
%%%%%%% Se si vuole usare un'altra codifica si veda sotto.
%%%%%%%

%%%%%%%  Esempio con molte opzioni
%%%%%%% Le opzioni nella forma "chiave=valore" sono definite
%%%%%%% perché la classe dalla versione 6.1.00 usa il pacchetto
%%%%%%% xkeyval. Vedere sulla documentazione in inglese o
%%%%%%% in italiano quali chiavi accettano valori.

%%%%%%% L'opzione per il corpo accetta qualsiasi valore, anche fratto
%%%%%%% (per esempio: corpo=11.5pt) e va sempre scritto con una
%%%%%%% unità di misura. L'utente è pregato di non esagerare con
%%%%%%% corpi normali minori di 9.5pt o maggiori di 13pt.
%%%%%%%
%%%%%%% Le opzioni per inputenc e fontenc vanno per prime.
%%%%%%% Vengono ignorate se NON si compone con pdfLaTeX. Ma
%%%%%%% questo è un esempio per pdfLaTeX.
%%%%%%%

 \documentclass[%
    corpo=11pt,
    twoside,
    stile=classica,
    oldstyle,
    autoretitolo,
    tipotesi=magistrale,
    greek,
    evenboxes,
    english
]{toptesi}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Per la codifica d'entrata si può scegliere quella che si vuole,
%%%%%% ma si consiglia di preferire utf8; in ogni caso non scegliere
%%%%%% codifiche specifiche del sistema operativo.

\usepackage[utf8]{inputenc}% codifica d'entrata
\usepackage[T1]{fontenc}%    codifica dei font
\usepackage{lmodern}%        scelta dei font


%\parindent=2mm
%\parskip=1mm

% Vedere la documentazione toptesi-it.pdf per le
% attenzioni che bisogna usare al fine di ottenere un file
% veramente conforme alle norme per l'archiviabilità.


\usepackage{hyperref}

\hypersetup{%
    pdfpagemode={UseOutlines},
    bookmarksopen,
    pdfstartview={FitH},
    colorlinks,
    linkcolor={blue},
    citecolor={blue},
    urlcolor={blue}
  }
%

\usepackage{appendix} %appendices
\usepackage{mathrsfs,amssymb,amsfonts,amsthm,amsmath,amsbsy}
\usepackage{natbib}
\newtheorem{theorem}{Theorem}[section]
%\newenvironment{theorem}{\begin{theorem}}{\end{theorem}}
\newtheorem{proposition}[theorem]{Proposition} 
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma} 
\newtheorem{example}[theorem]{Example} 
\newtheorem{definition1}[theorem]{Definition} \newenvironment{definition}{\begin{definition1}\rm}{\hfill$\square$\end{definition1}}
\newtheorem{remark1}[theorem]{Remark} \newenvironment{remark}{\begin{remark1}\rm}{\hfill$\square$\end{remark1}}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[font=small]{caption}
\usepackage{setspace} %spacing issues
%%%%%%%% Esempio di composizione di tesi di laurea con PDFLATEX
%
%
% Per scrivere testo fasullo in "latinorum"
%\usepackage{lipsum}
%

%%%%%%% Definizioni locali
\newtheorem{osservazione}{Osservazione}% Standard LaTeX
\newcommand{\sign}{\text{sign}}

\begin{document}\errorcontextlines=9
%%%%%%% Questi comandi è meglio metterli dentro l'ambiente
%%%%%%% frontespizio o frontespizio*, oppure in un file di
%%%%%%% configurazione personale. Si veda la documentazione
%%%%%%% inglese o italiana.
%%%%%%% Comunque i presenti comandi servono per comporre la
%%%%%%% tesi con i moduli di estensione standard del pacchetto
%%%%%%% TOPtesi.
\selectlanguage{english}

\thispagestyle{empty}

\centerline {\huge{\textsc{UNIVERSITY OF TORINO}}}
\vskip 30 pt

%\centerline {\Large{\textsc DIPARTIMENTO DI MATEMATICA GIUSEPPE PEANO}}
%\vskip 10 pt
%
%\centerline {\Large{\textsc DIPARTIMENTO ESOMAS}}
%\vskip 10 pt
%
%\centerline {\Large{\textsc DIPARTIMENTO DI INFORMATICA}}
%
%\vskip 30 pt
%
%%\centerline {{\textsc SCUOLA DI SCIENZE DELLA NATURA}}
%
%%\vskip 20 pt
%
\centerline {\LARGE{\textsc M.Sc. in Stochastics and Data Science}}
\vskip 30 pt

\centerline {\Large{\textsc Final dissertation}}
\vskip 50 pt





%\begin{tabular}{ccc}
\centerline {\includegraphics[width=4cm]{logounito.pdf}}
%   \end{tabular}

\vskip 1cm

%\centerline {\normalsize {Tesi di Laurea  Magistrale}}

\vskip 0.7cm

\begin{center} %THESIS TITLE
\Large \bf Rudiments of Neural Cryptography\\

\end{center}

\vskip 1.7cm
\large
\noindent  Supervisor: Elena Cordero  \hfill  {Candidate: Francesco Moraglio}\\
\noindent Co-supervisor: Stefano Barbero




\vskip 2.5cm


\centerline{ACADEMIC YEAR 2019/2020}



%%%%%%% Per cambiare l'offset per la rilegatura; meno offset
%%%%%%% c'e', meglio e'
%\setbindingcorrection{3mm}



\sommario

An overview of the main applications of neural networks to cryptography is presented. First two chapters are dedicated to 
theoretical foundations, while last three deal specifically with neural models, both for encryption and cryptanalisys. \\
More precisely, Chapter 1 is an introduction to the main ingredient of this recent branch of cryptography, that is neural networks. 
Special attention is given to the training algorithms that are typical for net architectures employed in this field. \\
Chapter 2 contains theoretical recalls about the design and the behavior of genetic algorithms; these interesting tools indeed
have applications in many fields including cryptography. \\
Chapter 3 is dedicated to the description of two early neural cryptosystems. Besides these models show the potential of neural networks
for encryption, their limitations are evident. Hence a treatment of such problems is also included. \\
Not only neural computational models are capable of creating secure communication channels, but they have recently been
showed efficient for cryptanalytic purpose. Chapter 4 contains two relevant examples of such applications. \\
Last Chapter is finally dedicated to what can be regarded as state-of-the-art neural cryptography, that is Adversarial Neural Cryptography. 
This cryptosystem was recently invented by Google researchers and solves the issues present in previous models. A section on extensions to this 
technique concludes the work.   
% \paginavuota % funziona anche senza specificare l'opzione classica

%\ringraziamenti

%You can insert here possible thanks and acknowledgements

\tablespagetrue\figurespagetrue % normalmente questa riga non serve ed e' commentata
\indici

%%%%%%%% fine esperimento
\mainmatter

\chapter{Introduction to Neural Networks}
First chapter of this work is dedicated to its main ingredient: Artificial Neural Networks. These are models of computation based loosely on the way in which the brain is believed to work. Since their invention, biologists are interested in using such networks to model biological brains, but most of the impetus comes from their employment in applied sciences: NN's allow us to create machine that can perform "cognitive" tasks.
\section{Biological Perspective}
To start off, an overview on biological neural network is needed, as ANN's in principle attempt to simulate them: figure \ref{fig:artificialneuron} gives a clear idea of this concept. An extensive treatment of both principles and applications of neural networks can be found in \cite{graupe}, to which I make reference for this chapter. \\
The biological neural network consists of interconnected nerve cells (called neurons), whose bodies are where most of the neural "computation" takes place. Neural activity passes from one neuron to another through electrical signals which travel from one cell to the following down the neuron's axon, that can be seen as a connection wire. However, the mechanism of signal transmission in not via electrical conduction, but via charge exchange that is transported by diffusion ions. At the terminal end of the axon, a synaptic gap takes place and an electro-chemical process allows communication with the other cell. \\
\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{pictures/neuron_biological_artificial.jpg}
\caption{Biological (above) and Artificial (below) neurons representation.}
\label{fig:artificialneuron}
\end{figure}
Since a given neuron may have many synapses, it can connect to many other cells. Similarly, since there are many dendrites (input connections) per each neuron, a single nerve cell can receive signals from many other neurons. It is very important to notice that not all of such interconnections are equally weighted: some have a higher priority than others. Also some are inhibitory and some are excitory. These facts are also key feature of artificial neural networks, as I will discuss in the following sections.
\newpage

\section{Principles}
\begin{singlespace}
The theoretical principles of the artificial neural networks first appeared in the pioneering paper \cite{mcculloch}. In this early work, five fundamental assumptions were formulated:
\end{singlespace}
\begin{enumerate}
\item the activity of an artificial neuron is all-or-nothing;
\item a certain fixed number of synapses greater than one must be excited for a neuron to be excited;
\item the only significant delay within the neural system is the synaptic delay;
\item the activity of any inhibitory synapse absolutely prevents the excitation of the neuron at that time;
\item the structure of the interconnection network does not change over time.
\end{enumerate}
Another widely applied principle is the so-called \textit{Hebbian Rule} (or \textit{Hebbian Learning Law}), that was first stated in \cite{hebb} as follows. \\
"When an axon of cell $A$ is near-enough to excite cell B and when it repeatedly and persistently takes part in firing it, then some growth process or metabolic change takes place in one or both these cells such that the efficiency of cell $A$ is increased". \\
Roughly speaking, such rule can be summarized as "cells that fire together wire together", that is the connections between two neurons might be strengthened (in terms of weights) if the neurons fire simultaneously. \\
Notice that above historical principles do not all apply to most modern neural network architectures. Following sections contain the description of the neural network models which have been employed (also in) neurocryptography.
\newpage
\section{The Perceptron}
\subsection{Basic structure}
\label{percbs}
The first complete neural computation model, that is called Perceptron, first appeared in \citep{rosenblatt} and serves as a building block to most later models. The Perceptron posseses the fundamental structure of a neural cell, of several weighted input connections and a single output channel. The input/output relations are defined to be
\begin{align}
z = \sum_{i}w_i x_i \quad \text{(summation output)} \label{summation}\\
y = f_N(z) \quad \text{(cell output)}, \label{nonlinear}
\end{align}
where $w_i$ is the weight at the input $x_i$. Such weights, according to the biological model, need to be adjustable. $y$, instead, denotes the output of the cell, that is a nonlinear activation function $f_N$ (see Subsection \ref{percact} below) of the node output $z$. \\
It is customary to consider a network of $n$ Perceptrons; in this case, by letting $z_i$ be the summation output of the $i$-th Perceptron and its inputs $x_{i,j}$, the summation relation becomes
\begin{equation}
z_i = \sum_{j=1}^{m}w_{ij} x_{ij}, \quad i \in \{1,\dots,n\},
\end{equation}
or, in vector form 
\begin{equation}
\label{percout}
z_i = \vec{w}_i^{\intercal}\vec{x}_i,
\end{equation}
where
\begin{align*}
\vec{w}_i = \left[w_{i1}, \dots, w_{in} \right]^\intercal \\
\vec{x}_i = \left[x_{i1}, \dots, x_{in} \right]^\intercal,
\end{align*}
for every $i \in \{1,\dots,n\}$. 
\subsection{Activation functions}
\label{percact}
\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{pictures/sigmoid.png}
\caption{The sigmoid curve.}
\label{fig:sigmoid}
\end{figure}
The cell output of the Perceptron differs from the summation output \ref{percout} by the activation operation of the neuron's body, just as the output of the biological cell differs from the weighted sum of its inputs. Such operation is in terms of an activation function $f_N(z_i)$, which is a non-linear operator yielding the output of $i$-th neuron $y_i$ to satisfy certain limiting properties. Different functions are in use, but the most common choice is the sigmoid function, that is
\begin{equation}
\label{act-sigmoid}
y_i = f_N(z_i) = \frac{1}{1 + e^{-z_i}}, \quad i \in \{1,\dots,n\}.
\end{equation}
Under this choice, the following relations are satisfied (see figure \ref{fig:sigmoid}).
\begin{align*}
z_i \to - \infty \quad &\Longleftrightarrow \quad y_i \to 0 ;\\
z_i = 0 \quad &\Longleftrightarrow \quad y_i = 0.5; \\
z_i \to  \infty \quad &\Longleftrightarrow \quad y_i \to 1.
\end{align*}
Another popular activation function is
\begin{equation}
\label{act-tanh}
y_i = \frac{1 + \tanh(z_i)}{2} = \frac{1}{1 + e^{-2z_i}},
\end{equation}
whose shape is similar to the one of the previous function. The simplest activation one could choose is the Heaviside step function:
\begin{equation*}
y_i = H(z_i) = \begin{cases}
1 \quad \text{if} \quad z_i \geq 0 \\
0 \quad \text{otherwise.}
\end{cases}
\end{equation*}
Main advantage of the former two over the latter is that they are (mathematically) smooth. \\
In many applications the output of the activation function is translated so that it ranges in $\left(-1, 1\right)$, rather than in $\left(0, 1 \right)$. In particular, sigmoid function \eqref{act-sigmoid} becomes 
\begin{equation}
y_i = \frac{2}{1 + e^{-z_i}} -1 = \tanh(z_i/2),
\end{equation}
while activation \eqref{act-tanh} is transformed to 
\begin{equation}
y_i = \tanh(z_i) = \frac{1 - e^{-2z_i}}{1 + e^{-2z_i}}.
\end{equation}
We recall that in many neural network models, a bias term is added to the summation output, so that \eqref{summation} becomes:
\begin{equation}
z = b + \sum_{i}w_i x_i,
\end{equation}
where $b$ is subject to the same training procedures as the the weights of the net. \\ Such training algorithms revealed themselves to be challenging for researchers, especially from the introduction of the so called Multi-Layer Perceptron (MLP). In fact, already in the late 1960s, limitations of single-layer architectures were shown (see \cite{minpapert}), together with the possibility of solving more complicated problems using MLP's. However, finding efficient algorithms to adjust the weights of such nets was problematic and took several years. Some examples of solutions to these problems are presented in the following sections. All of such algorithms are thought to work in discrete time, while taking into account partial derivatives of the quantities of interest. Please read appendix \ref{fdmappendix} for further detail about Finite Difference Methods (FDMs).

\subsection{Least Mean Square Training}
We first discuss a simple algorithm to set weights of the so-called Adaline (ADAptive LInear NEuron) network. Such model first appeared in \cite{widrow} and it has the basic structure of a bipolar perceptron. The nonlinear operator of equation \eqref{nonlinear} is here a simple threshold element, to yield the Adaline output $y$ as
\begin{equation}
y = \sign(z).
\end{equation}
The training of such NN is according to the following procedure, that is called Least Mean Square (LMS). Given $L$ training sets:
\begin{equation}
\vec{x}_1,\dots,\vec{x}_L; \quad d_1, \dots, d_L,
\end{equation}
where
\begin{equation}
\vec{x}_i = \left[x_{i1}, \dots, x_{in} \right]^\intercal, \quad i \in \{1,\dots, L\},
\end{equation}
$i$ denoting the $i$-th set, $n$ being the number of inputs and $d_i$ denoting the desired output of the neuron, we define a training cost, such that:
\begin{align}
J(\vec{w}) &= \mathbb{E}\left[ e_k^2\right] \approx \frac{1}{L}\sum_{k=1}^{L}e_k^2; \label{trainingcost} \\
\vec{w} &= \left[w_{1}, \dots, w_{n} \right]^\intercal,
\end{align}
where $e_k$ denotes training error at the $k$-th set, namely
\begin{equation}
e_k = d_k - z_k, \quad k \in \{1,\dots, L\},
\end{equation}
and $z_k$ being the neuron's actual output. \\
Following the above notation we have that 
\begin{equation}
\mathbb{E}\left[e_k^2\right] = \mathbb{E}\left[d_k^2\right] + \vec{w}^{\intercal}\mathbb{E}\left[\vec{x}_k\vec{x}_k^{\intercal}\right]\vec{w} - 2\vec{w}^{\intercal}\mathbb{E}\left[d_k\vec{x}_k\right],
\end{equation}
with
\begin{align}
\mathbb{E}\left[XX^{\intercal} \right] &= R \label{expectdesign}\\
\mathbb{E}\left[\vec{d}X\right] = \vec{p},
\end{align}
where $X$ denotes the input matrix, to yield the gradient of $J$ such that
\begin{equation}
\label{lmsgrad}
\nabla J = \frac{\partial J(\vec{w})}{\partial \vec{w}} = 2R\vec{w} - 2\vec{p}.
\end{equation}
Hence, the optimal LMS setting of the weights becomes
\begin{equation}
\nabla J = 0,
\end{equation}
which, by equation \eqref{lmsgrad} satisfies the weight setting of 
\begin{equation}
\vec{w}^{LMS} = R^{-1}\vec{p}.
\end{equation}
Note that above LMS procedure employs expecting whereas the training data is limited to a small number of $L$ sets, such that the sample averages will be inaccurate estimates of the true expectations. 

\subsection{Gradient Descent Training}
\label{gdtsection}
The gradient descent training procedure, also known in literature as steepest descent procedure, attempts to provide weight-setting estimates from one training set to the next, starting with $L=n+1$ training sets, where $n$ is the number of inputs. Note that, from $n$ weights, it is imperative that 
\begin{equation}
L > n + 1
\end{equation}
Denoting a weights vector setting after the $m$-th training set, $m \in \{1, \dots, L\}$, as $\vec{w}(m)$, then 
\begin{equation}
\label{steepestdescent}
\vec{w}(m+1) = \vec{w}(m) + \Delta\vec{w}(m),
\end{equation}
where $\Delta\vec{w}$ is the variation in $\vec{w}(m)$, given by
\begin{equation}
\label{variation}
\Delta\vec{w}(m) = \mu \nabla J\left(\vec{w}(m)\right).
\end{equation}
In the above equation, $\mu$ is the rate parameter whose setting discussed below, and $\nabla J$ denotes the gradient of the training cost as defined in \eqref{trainingcost}. With these conventions, the steepest descent procedure of \eqref{steepestdescent} follows the steps below.
\begin{enumerate}
\item Apply input vector $\vec{x}_m$ and the desired output $d_m$ for the $m$-th training set .
\item Determine $e_m^2$, that is
\begin{align}
e_m^2  &= \left[d_m - \vec{w}(m)^{\intercal}\vec{x}(m) \right]^2 =  \\ 
& = d_m^2 -2d_m\vec{w}(m)^{\intercal}\vec{x}(m) + \vec{w}(m)^{\intercal}\vec{x}(m)\vec{x}(m)^{\intercal}\vec{w}(m)
\end{align}

\item Evaluate
\begin{align}
\nabla J &= \frac{\partial e_m^2}{\partial \vec{w}(m)} = 2\vec{x}(m)\vec{w}(m)^{\intercal}\vec{x}(m) - 2d_m\vec{x}(m) = \\
		&= -2\left[d_m - \vec{w}(m)^{\intercal}\vec{x}(m) \right]\vec{x}(m) = -2e_m\vec{x}(m),
\end{align}
thus obtaining an approximation to $\nabla J$ by using $e_m^2$ as the approximate to $J$.

\item Update $\vec{w}(m+1)$ via equations \eqref{steepestdescent} and \eqref{variation}, namely
\begin{equation}
\vec{w}(m+1) = \vec{w}(m) -2\mu e_m\vec{x}(m).
\end{equation}
Here $\mu$ is chosen to satisfy 
\begin{equation}
\frac{1}{\lambda_{max}} > \mu > 0,
\end{equation}
where $\lambda_{max}$ is the maximum eigenvalue of matrix $R$ of equation \eqref{expectdesign}.
\end{enumerate}
In the following sections we will discuss more advanced training algorithms, that are designed to be compatible with multiple-layer architectures.
\newpage
\section{Back Propagation Training Algorithm}
Back Propagation (BP) algorithm was first proposed in \cite{back}, as a solution for setting weights (and hence for the training) of multi-layer perceptrons. 
The availability of a rigorous method to set intermediate weights, namely to train the hidden layers of neural networks, gave a major boost to the further development of such models. \\
In this section this algorithm is presented in detail. \\
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{pictures/multilayer-perceptron.png}
\caption{Schematic representation of multi-layer perceptron.}
\label{fig:mlp}
\end{figure}
\subsection{The Algorithm}
The BP algorithm starts with computing values of the output layer, which is by definition the only one whose desired outputs are available. Let $\epsilon$ be the the \textit{error-energy} at the output layer, that is
\begin{equation}
\label{errorenergy}
\epsilon = \frac{1}{2}\sum_{k=1}^{N}e_k^2  =\frac{1}{2}\sum_{k=1}^{N}\left(d_k - y_k\right)^2,
\end{equation}
where $N$ is the number of neurons in the output layer. We also recall $d_k$ is the desired output and $e_k$ the error at the $k$-th entry . Consider the gradient of $\epsilon$:
\begin{equation}
\nabla \epsilon_k = \frac{\partial \epsilon}{\partial w_{kj}}.
\end{equation}
Recall that $w_{kj}$ denotes the weight of the $j$-th input to the $k$-th neuron. By the gradient descent procedure (see Subsection \ref{gdtsection}), we have that 
\begin{equation}
w_{kj}(m+1) = w_{kj}(m) + \Delta w_{kj}(m),
\end{equation}
where 
\begin{equation}
\label{deltasub}
\Delta w_{kj} = -\eta \frac{\partial \epsilon}{\partial w_{kj}}
\end{equation}
and $\eta \in \left(0,1 \right)$ is the rate parameter. Note that the minus sign indicates a down-hill direction towards a minimum. \\
With the notations of Subsection \ref{percbs}, we can now substitute 
\begin{equation}
\label{substitution}
\frac{\partial \epsilon}{\partial w_{kj}} = \frac{\partial \epsilon}{\partial z_k} \frac{\partial z_k}{\partial w_{kj}}
\end{equation}
and 
\begin{equation}
\label{graupe6p8}
\frac{\partial z_k}{\partial w_{kj}} = x_j(p) = y_j(p -1),
\end{equation}
with $p$ denoting the output layer. This way, equation \eqref{substitution} becomes 
\begin{equation}
\label{newsubstitution}
\frac{\partial \epsilon}{\partial w_{kj}} = \frac{\partial \epsilon}{\partial z_k}x_j(p) = \frac{\partial \epsilon}{\partial z_k} y_j(p-1).
\end{equation}
We now have to define 
\begin{equation}
\label{phisub}
\phi_k(p) = - \frac{\partial \epsilon}{\partial z_k},
\end{equation}
so that \eqref{newsubstitution} yields 
\begin{equation}
\frac{\partial \epsilon}{\partial w_{kj}} = - \phi_k(p)x_j(p) = -\phi_k(p)y_j(p-1).
\end{equation}
So by this last equation and \eqref{deltasub} we get
\begin{equation}
\label{graupe6p12}
\Delta w_{kj} = \eta \phi_k(p)x_j(p) = \eta \phi_k(p)y_j(p-1).
\end{equation}
Furthermore, by \eqref{phisub}:
\begin{equation}
\label{newphi}
\phi_k = - \frac{\partial \epsilon}{\partial z_k} = - \frac{\partial \epsilon}{\partial y_k} \frac{\partial y_k}{\partial z_k}.
\end{equation}
But, going back to error-energy definition in \eqref{errorenergy}
\begin{equation}
\frac{\partial \epsilon}{\partial y_k} = -\left(d_k - y_k \right) = y_k - d_k,
\end{equation}
where we recall 
\begin{equation}
y_k = f_N(z_k) = \frac{1}{1 + e^{-z_k}}
\end{equation}
and so we have that 
\begin{equation}
\label{graupe6p16}
\frac{\partial y_k}{\partial z_k} = y_k(1 - y_k).
\end{equation}
Consequently, by equations \eqref{newphi} and subsequent ones, 
\begin{equation}
\label{lastphi}
\phi_k = y_k(1 - y_k)(d_k - y_k),
\end{equation}
such that, at the output layer, by \eqref{deltasub} and \eqref{substitution}
\begin{equation}
\Delta w_{kj} = -\eta \frac{\partial \epsilon}{\partial w_{kj}} = -\eta  \frac{\partial \epsilon}{\partial z_k} \frac{\partial z_k}{\partial w_{kj}},
\end{equation}
where, by \eqref{newphi}
\begin{equation}
\Delta w_{kj}(p) = \eta \phi_k(p)y_j(p-1),
\end{equation}
with $\phi_k$ being as in \eqref{lastphi}, to complete the derivation of the setting of output layer's weights. \\
If we now consider, in general, the $i$-th input to the $j$-th neuron of the $r$-th hidden layer, we still have, as before
\begin{equation}
\Delta w_{ji} = -\eta \frac{\partial \epsilon}{\partial w_{ij}},
\quad j \in \{1,\dots,N\}; \quad i \in\{1, \dots, n \}.
\end{equation}
Similarly to \eqref{substitution} it holds
\begin{equation}
\frac{\partial \epsilon}{\partial w_{ji}} = \frac{\partial \epsilon}{\partial z_j} \frac{\partial z_j}{\partial w_{ji}}
\end{equation}
and taking into account equation in \eqref{graupe6p8} and the expression of $\phi$ in \eqref{newphi} we get
\begin{equation}
\Delta w_{ji} = -\eta \frac{\partial \epsilon}{\partial z_j} y_i (r-1) = \eta \phi_j(r)y_i(r-1).
\end{equation}
This way, again by right-hand side of \eqref{newphi}, we obtain
\begin{equation}
\Delta w_{ji} = -\eta \left[ \frac{\partial \epsilon}{\partial y_j (r)} \frac{\partial y_j}{\partial z_j} \right]y_i(r-1),
\end{equation}
where $\frac{\partial \epsilon}{\partial y_j}$ is inaccessible (just like $\phi_j (r)$ above). \\
Since we are considering backward propagation from the output, error-energy is only determined by upwards neurons:
\begin{equation}
\frac{\partial \epsilon}{\partial y_j (r)} = \sum_k \frac{\partial \epsilon}{\partial z_k (r+1)} \left[\frac{\partial z_k (r+1)}{\partial y_j (r)} \right] = \sum_k \frac{\partial \epsilon}{\partial z_k} \left[ \frac{\partial}{\partial y_j(r)} \sum_m w_{km}(r+1)y_m(r)\right].
\end{equation}
We remark that, in above expression, summation over $k$ is over the neurons of layer $r+1$ that connect to $y_j(r)$, while summation over $m$ is performed over all inputs to each $k$-th neuron of the $(r+1)$-th layer. Keeping in mind the definition of $\phi$, last equation yields
\begin{equation} 
\label{graupe6p25}
\frac{\partial \epsilon}{\partial y_j (r)}  = \sum_k \frac{\partial \epsilon}{\partial z_k (r+1)}w_{kj} = - \sum_k \phi_k(r+1)w_{kj}(r+1),
\end{equation}
since only $w_{kj}(r+1)$ is connected to $y_j(r)$. Consequently, by equations \eqref{newphi}, \eqref{graupe6p16} and \eqref{graupe6p25}
\begin{equation}
\phi_j(r) = \frac{\partial y_j}{\partial z_j}\sum_k \phi_k(r+1)w_{kj}(r +1) = y_j(r)\left[1 -y_j(r)\right]\sum_k \phi_k(r+1)w_{kj}(r +1)
\end{equation}
and finally, with some other substitutions, we obtain
\begin{equation}
\Delta w_{ji}(r) = \eta \phi_j(r)y_i(r-1),
\end{equation}
that is a function of $\phi$ and of the weights of the $(r + 1)$-th layer. \\
Back Propagation algorithm thus propagates up to the first layer ($r = 1$), iterating over a pre-fixed set of training vectors. Weights are initialized randomly, while the learning rate $\eta$ should be adjusted stepwise for faster convergence.
\subsection{Refinements and Extensions of BP}
A series of modifications to the Back Propagation algorithm were introduced by scientists in order to improve the training procedure. Most important extensions include the introduction of bias into neural networks, smoothing the weight adjustment function and rescaling the activation function. \\
If we want to consider bias in a model, we can regard it as a trainable quantity, just as is any other weight. In general, the bias term $b_i$ at the input to the $i$-th neuron is realized in term of a constant input $B$ with its relative weight:
\begin{equation}
b_i = w^b_i B, \quad i \in \{1,\dots,N\}.
\end{equation}
This algorithm sometimes tends to go through instability issues when computing weights and several smoothing techniques were proposed to deal with such problems. For instance, in \cite{sejnowski}, a smoothing term to modify equation \eqref{graupe6p12} is given:
\begin{align}
&\Delta w_{ij}^{(m)} = \alpha \Delta w_{ij}^{(m-1)} + (1-\alpha)\phi_i(r)y_j(r-1)  \nonumber \\
& w_{ij}^{(m+1)} = w_{ij}^{(m)} + \eta \Delta w_{ij}^{(m)},
\end{align}
where $\alpha \in \left(0,1 \right)$.
Last, modifying the range of the sigmoid function, typically to $\left(-0.5,0.5\right)$ seems to improve the convergence rate of BP algorithm. 
\newpage 

\section{Adam}
In this section we present Adam, a recent method for stochastic optimization that first appeared in \cite{adam}. This method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation. 
Such algorithm is well-suited for the training of neural networks and indeed it is applied to most modern architectures; following chapters of this work include examples of such NNs.
\subsection{The Algorithm}
Here the theory of \textit{Adam} optimizer is presented according to the original publication  where it was first proposed. Pseudocode for this algorithm is reported below. \\
\begin{algorithm}[H]
\label{algo:adam}
\SetAlgoLined
%\KwResult{Write here the result }
 \textbf{Require:} $\alpha$: Stepsize\;
 \textbf{Require:} $\beta_1,\beta_2 \in \left[0,1\right)$: Exponential decay rates\;
 \textbf{Require:} $f(\theta)$: Stochastic objective function with parameters $\theta$\;
 \textbf{Require:} $\theta_0$: Initial parameter vector\;
 $m_0 \leftarrow 0$ (Initialize first moment vector)\;
 $v_0 \leftarrow 0$ (Initialize second moment vector)\;
 $t \leftarrow 0$ (Initialize time step)\;
 \While{$\theta_t$ not converged}{
  $t \leftarrow t + 1$\;
  $g_t \leftarrow \nabla_{\theta}f_t(\theta_{t-1})$ (Get gradients)\; 
  $m_t \leftarrow \beta_1m_{t-1} + (1-\beta_1)g_t$ (Biased first moment est.)\;
  $v_t \leftarrow \beta_2v_{t-1} + (1 -\beta_2)g_t^2$ (Biased second moment est.)\;
  $\hat{m}_t \leftarrow 	\frac{m_t}{1 - \beta_1^t}$ (Bias-corrected first moment est.)\;
  $\hat{v}_t \leftarrow 	\frac{v_t}{1 - \beta_2^t}$ (Bias-corrected second moment est.)\;
  $\theta_t \leftarrow \theta_{t-1} - \alpha\frac{\hat{m}_t}{\sqrt{\hat{v}_t}+\epsilon}$ (Update parameters)\;
 }
 \Return{$\theta_t$}
 \caption{\textit{Adam} stochastic optimization algorithm. $g_t^{2}$ indicates the elementwise square $g_t \cdot g_t$.}
\end{algorithm}

Let $f(\theta)$ be a noisy objective function: a stochastic scalar function that is differentiable with respect to parameters $\theta$. The stochasticity might come from the evaluation at random subsamples (minibatches)of datapoints, or arise from inherent function noise.\\
We are interested in minimizing the expected value of such function, that is $\mathbb{E}\left[f(\theta)\right]$. Denote by $f_1(\theta), \dots,f_T(\theta)$ the realizations of the function at subsequent time steps $1,\dots, T$. Let $g_t = \nabla_{\theta}f_t(\theta)$ be the gradient, that is the vector of partial derivatives of $f$ (w.r.t. $\theta$) evaluated at time step $t$. \\
The algorithms updates exponential moving averages of the gradient and of the squared gradient; denote these estimates by $m_t$ and $v_t$, respectively. Exponential decay rates of these moving averages are controlled by the hyperparameters $\beta_1,\beta_2 \in \left[0,1\right)$. The moving average themselves are estimates of the mean and the uncentered variance (the second moment) of the gradient. Since these estimates are initialized as vectors of zeros, they are biased towards zero, especially during the initial time steps and when the decay rates are small (i.e. $\beta_1,\beta_2 \approx 1$). This initialization bias can however be easily counteracted, resulting in bias-corrected estimates $\hat{m}_t$ and $\hat{v}_t$, as described in the following. \\
Recall that $g_1,\dots,g_T$ denote the gradients at subsequent time steps; each can be seen as a draw from an underlying gradient distribution $p(g_t)$. Consider the second moment estimate. As can be seen in pseudocode \ref{algo:adam}, the exponential moving average is initialized as $v_0 = 0$. First note that the update at time step $t$, $v_t = \beta_2v_{t-1} + (1 -\beta_2)g_t^{2}$, can be written as a function of the gradients at all previous time steps:
\begin{equation}
\label{adam:update}
v_t = (1 - \beta_2)\sum_{i=1}^{t}\beta_2^{t-i}g_i^2.
\end{equation}
We wish to know how $\mathbb{E}\left[v_t\right]$, the expected value of the exponential moving average at time step $t$, relates to the the true second moment $\mathbb{E}\left[g_t^2 \right]$, so we can correct for discrepancy between the two. Taking expectations of both sides of equation \eqref{adam:update}, we have
\begin{align}
\mathbb{E}\left[v_t\right] &= \mathbb{E}\left[ (1 - \beta_2)\sum_{i=1}^{t}\beta_2^{t-i}g_i^2\right] = \\
&= \mathbb{E}\left[g_t^2 \right](1 - \beta_2)\sum_{i=1}^{t}\beta_2^{t-i} + \zeta = \\
&= \mathbb{E}\left[g_t^2 \right](1 - \beta_2^t)\ + \zeta,
\end{align}
where $\zeta = 0$ if the true second moment $\mathbb{E}\left[g_t^2 \right]$ is stationary. If not, $\zeta$ can be kept small since the exponential decay rate $\beta_2$ can (and should) be chosen such that the exponential moving average assigns small weights to gradients far in the past. What is left is the term $(1 - \beta_2^t)$ which is caused by initializing the running average with zero. In algorithm \ref{algo:adam} we therefore divide by this term to correct the initialization bias. \\
Going back to \textit{Adam}'s update rule, we have to focus on its careful choice of stepsizes. Assuming $\epsilon = 0$, the effective step taken in the parameter space at time step $t$ is 
\begin{equation}
\Delta_t = \alpha\frac{\hat{m}_t}{\sqrt{\hat{v}_t}}.
\end{equation}
The effective step size has two upper bounds:
\begin{equation}
\begin{cases}
|\Delta_t| \leq \alpha\frac{1 - \beta_1}{\sqrt{1 - \beta_2}} \quad &\text{if} \quad (1-\beta_1) > \sqrt{1 - \beta_2}; \\
|\Delta_t| \leq \alpha \quad &\text{otherwise.}
\end{cases}
\end{equation}
The first case only happens in the most severe case of sparsity: when the gradient has been zero at all time steps except at the current time step. Otherwise, the effective step size will be smaller. In more common scenarios, we will have that 
$$\frac{\hat{m}_t}{\sqrt{v_t}} \approx \pm 1,$$
since 
$$\frac{|\mathbb{E}\left[g\right]|}{\sqrt{\mathbb{E}\left[g^2 \right]}}\leq 1.$$
Hence the effective magnitude of the steps taken in the parameter space at each time step are approximately bounded by the stepsize setting $\alpha$. \\
The effective step size $\Delta_t$ is also invariant to the scale of the gradients; rescaling the gradients $g$ with factor $c$ will not lead to changes:
\begin{equation}
\frac{c\hat{m}_t}{\sqrt{c^2\hat{v}_t}} = \frac{\hat{m}_t}{\sqrt{\hat{v_t}}}.
\end{equation}
\chapter{Genetic Algorithms: a brief overview}
Genetic Algorithms were invented by John Holland in the 1960s and can be considered a key building block of artificial intelligence. His aims were that of  formally studying the phenomenon of natural adaptation and that of importing such adaptation mechanisms into computer systems. In \cite{holland}, Genetic Algorithms (GA's) are presented as abstractions of biological evolution. Moreover, a theoretical framework for adaptation under GA's is described. \\
Holland's GA is a method for evolving from one population of "chromosomes" to a new population by using an imitation of natural selection together with the genetic-inspired operators of crossover and mutation. In this chapter we briefly discuss how GA's works and why. Main reference for this part is \cite{mit}, whose author was one of Holland's students at MIT.
\section{Definitions and Setting}
The genome of the evolutionary model is represented by a multiset of size $N$:
\begin{equation}
\textbf{P}(t) = \{\vec{v}_1, \dots, \vec{v}_N \},
\end{equation}
whose elements (that are not necessarily distinct) are called chromosomes. Time variable $t\in \mathbb{N}$ denotes the $t$-th generation of the population. Each chromosome $\vec{v}_i$ is a bit vector of fixed length $n$:
\begin{equation}
\vec{v}_i = \left[v_i^{(1)}, \dots, v_i^{(n)} \right], \quad i \in \{1, \dots, N\},
\end{equation}
where $v_i^{(j)} \in \{0,1\}$, $\forall j = 1, \dots, n$. We recall that there exist more advanced ways to represent chromosomes, but here we are limiting to the binary representation for simplicity. \\
We now need to define some probabilities which are involved in the GA. Selection is based on the definition of a fitness function over the population:
\begin{equation}
f: \textbf{P}(t) \longrightarrow \mathbb{R}_0^{+}.
\end{equation}
We are now allowed to define total fitness and average fitness, namely
\begin{align}
F &= \sum_{\vec{v}\in \textbf{P}(t)}f(\vec{v}) \quad \text{and} \\
\overline{F} &= \frac{F}{N}, \label{avgfit}
\end{align}
respectively. We can hence define the selection probability $P_S(\vec{v})$, for each chromosome $\vec{v}$:
\begin{equation}
P_S(\vec{v}) = \frac{f(v)}{F}.
\end{equation}
Note that \eqref{avgfit}, despite not being explicitly computed in the algorithm, has great theoretical relevance (see Subsection \ref{gabehavior}). \\
The other probabilistic quantities involved are crossover probability $P_C$ and mutation probability $P_M$. The former can also be regarded as the fraction of elements of $\textbf{P}(t)$ that will go through reproduction, while the latter can be thought of as the percentage of bits (of all chromosomes) that will change due to random mutations. Both values are fixed and typically we have $P_C >> P_M$.
\newpage

\section{The Algorithm}
Each execution of the Genetic Algorithm produces an offspring of chromosomes: it moves from  population $\textbf{P}(t)$ to $\textbf{P}(t+1)$. This evolution process happens in three main phases:
\begin{enumerate}
\item selection via \textit{Wheel of Fortune} model;
\item reproduction via crossover;
\item mutation.
\end{enumerate}
\begin{figure}[h]
\centering
\includegraphics[scale = 0.25]{pictures/genetic_algo.png}
\caption{Flow chart representation of Genetic Algorithm.}
\label{fig:genalgo}
\end{figure}
\subsection{Selection}
As anticipated just above, in the first phase the algorithm mimics the behavior of wheel of fortune. First, a sequence $\{q_i\}_{i=0}^{N}$ is computed recursively in the following way:
\begin{align}
q_0 &= 0, \nonumber \\
q_1 &= P_S(\vec{v}_1), \nonumber \\
q_2 &= P_S(\vec{v}_1) + P_S(\vec{v}_2), \nonumber \\
&\vdots \nonumber \\
q_i &= \sum_{j=1}^{i}P_S(\vec{v}_j), \nonumber \\
&\vdots \nonumber \\
q_N &= \sum_{j=1}^{N}P_S(\vec{v}_j) = 1.
\end{align}
Such sequence determines a partition of the interval $\left[0,1\right]$ and the algorithm proceeds by "spinning" the wheel of fortune $N$ times . This means, a random number $r\in \left[0,1\right]$ is generated; if $r \leq 1$, then $\vec{v}_1$ is chosen; else if $q_{j-1} < r \leq q_j$ the chosen chromosome is $\vec{v}_j$. Notice that, with this procedure, a single chromosome may be selected several times. 
\subsection{Reproduction}
Second phase simulates biological crossover, thanks to which genetic recombination is guaranteed at every generation. For each chromosome $\vec{v}$ selected in previous phase
\begin{itemize}
\item a random number  $r\in \left[0,1\right]$ is generated;
\item if $r < P_C$, then $\vec{v}$ is chosen for crossover.
\end{itemize}
Recall that $P_C$ is a fixed selection probability. Chromosomes chosen this way are then randomly coupled. Whenever their number is odd, another random chromosome can be added (or removed from the reproduction subset).\\
For each couple $\vec{v}_A = \left[v_A^{(1)}, \dots, v_A^{(n)} \right] $ and $\vec{v}_B = \left[v_B^{(1)}, \dots, v_B^{(n)} \right] $, a random integer 
$c \in \{1, 2, \dots, n - 1 \}$ is computed and the chromosomes are split in the following way:
\begin{align}
\vec{v}_A &= \left[v_A^{(1)}, \dots, v_A^{(c)} | v_A^{(c+1)}, \dots,  v_A^{(n)} \right]; \nonumber \\
\vec{v}_B &= \left[v_B^{(1)}, \dots, v_B^{(c)} | v_B^{(c+1)}, \dots,  v_B^{(n)} \right].
\end{align}
After that, offspring chromosomes $\vec{w}_A$ and $\vec{w}_B$ are generated by swapping the blocks found above:
\begin{align}
\vec{w}_A &= \left[v_A^{(1)}, \dots, v_A^{(c)} | v_B^{(c+1)}, \dots,  v_B^{(n)} \right]; \nonumber \\
\vec{w}_B &= \left[v_B^{(1)}, \dots, v_B^{(c)} | v_A^{(c+1)}, \dots,  v_A^{(n)} \right].
\end{align}
In other words
\begin{equation}
\vec{w}_A = \begin{cases}
v_A^{(i)} \quad \text{if} \quad 1 \leq i \leq c \\
v_B^{(i)} \quad \text{if} \quad c + 1 \leq i \leq n;
\end{cases} 
\end{equation}
and
\begin{equation}
\vec{w}_B = \begin{cases}
v_B^{(i)} \quad \text{if} \quad 1 \leq i \leq c \\
v_A^{(i)} \quad \text{if} \quad c + 1 \leq i \leq n.
\end{cases}
\end{equation}
\subsection{Mutation}
For each chromosome $\vec{v}_i$ we consider its every bit $v_i^{(j)}$, $\forall j=1,\dots,n$ and the algorithm generates a random $r_j \in \left[0,1\right]$.
If $r_j < P_M$, then the corresponding bit is changed to its two's complement (otherwise it's left unchanged), where we recall that $P_M$ denotes the mutation probability. After these three phases we obtain a new population $\textbf{P}(t+1)$ with 
\begin{equation}
|\textbf{P}(t+1)|=|\textbf{P}(t)|=N.
\end{equation}
\newpage

\section{Holland Theorem}
Despite GA's are quite simple to describe and implement, understanding their behavior appears to be quite complicated. According to the theory of Holland, such algorithms work by discovering, emphasizing and recombining good "building blocks" (taken from chromosomes with high fitness). Such building blocks are formally named "schemas". 
\subsection{Definitions}
A schema is a set of bit strings that can be described as a template made up of the symbols $0$, $1$ and $*$, which denotes a "free entry". For example, the schema
\begin{equation}
\label{schemah}
K = 1**1
\end{equation}
represents the set of all four-bit strings that begin and end with $1$. Vectors belonging to one of such sets are named instances of that schema. \\
For a scheme $H$, we denote $o(H)$ the order of $H$, that is its number of defined bits ($\neq *$) and we let $d(H)$ be the schema's length (the distance between its outermost defined bits). In example before \eqref{schemah}, we have $o(K)=2$ and $d(K)=3$.
\subsection{Behavior of Genetic Algorithms}
\label{gabehavior}
After this few premises, we can discuss how the GA processes schemas. \\
Any given bit string of length $l$ in an instance of $2^l$ different schemas. Thus, given a population of $N$ chromosomes, it contains instances of $k$ different schemes, with $2^l \leq k \leq N2^l$. This means that, at a given generation $t$, while the GA is explicitly evaluating the fitness of the $N$ chromosomes, it is actually implicitly estimating the average fitness of a much larger number of schemas (where the average fitness of a schema is defined to be the average fitness of all possible instances of that schema). Obviously, the estimates of schema average fitness are not calculated or stored by the GA, However, the behavior of the algorithm can be described as though it was actually calculating and storing these averages. \\
Let $H$ be a schema with at least one instance present in $\textbf{P}(t)$ and let $m(H,t)$ the number of instances of $H$ at time $t$. Moreover let $\hat{U}(H,t)$ be the observed average fitness. We want to compute 
\begin{equation}
\mathbb{E}\left[m(H, t + 1) \right],
\end{equation}
the expected number of instances of $H$ at time $t+1$. Assume that selection is carried out as described in previous section; the expected number of offspring of a chromosome $\vec{v}$ is equal to
\begin{equation}
\frac{f(\vec{v})}{\overline{F}}.
\end{equation}
We will write $\vec{v} \in H$ to denote "$\vec{v}$ is an instance of $H$". \\
Ignoring (only for now) the effects of crossover and mutation, we have
\begin{equation}
\label{expectedmult}
\mathbb{E}\left[m(H, t + 1) \right] = \sum_{\vec{v} \in H}\frac{f(\vec{v})}{\overline{F}} = \frac{\hat{U}(H,t)}{\overline{F}}m(H,t)
\end{equation}
by definition, since
\begin{equation}
\hat{U}(H,t) = \frac{\sum_{\vec{v} \in H}f(\vec{v})}{m(H,t)}.
\end{equation}
Thus even though the GA does not compute $\hat{U}(H,t)$ explicitly, the propagation of schema instances in the population depends on this quantity. \\
Crossover and mutation can both destroy and create instances of $H$, but we limit to considering the disruptive effects. We say schema $H$ "survives" under crossover if at least one instance of such schema is present in the next generation. Letting $S_C(H)$ be the probability of such survival, with the customary notations we have
\begin{equation}
S_C(H) \geq 1 - P_C\left(\frac{d(H)}{n-1} \right).
\end{equation}
In short, the probability of survival under crossover is higher for short schemas. \\
The effects of mutations can be quantified as follows. Let $S_M(H)$ be the probability that schema $H$ will survive under mutation. It can be expressed as
\begin{equation}
S_M(H) = (1-P_M)^{o(H)},
\end{equation}
where we recall that $o(H)$ is the number of defined bits in the schema. This formula holds because mutation acts independently on each bit of each chromosome. In this case, longer schemes appear to be more resistant. \\
These disruptive effects can be used to amend equation \eqref{expectedmult}, in order to get a lower bound for $\mathbb{E}\left[m(H, t + 1) \right]$. Hence we get
\begin{equation}
\mathbb{E}\left[m(H, t + 1) \right] \geq \frac{\hat{U}(H,t)}{\overline{F}}m(H,t)\left( 1 - P_C\left(\frac{d(H)}{n-1} \right)\right)\left[ (1-P_M)^{o(H)}\right].
\end{equation}
Last result is known as Holland Theorem (or Schemata Theorem). It implies that short, high-average-fitness schemas will see their instances increase exponentially over time.

\chapter{Neural Cryptography: History}
\section{Introduction}
Cryptography is one of the most crucial aspects for cybersecurity and it is becoming increasingly indispensable in information age. In classical cryptosystems, cryptography algorithms are mostly based on classical hard-to-solve problems in number theory. \\
However, several researchers have combined neural network with classical cryptography for the multivariate structural and nondirectional features of neural network. More precisely, the concept of cryptography based on NNs was firstly introduced in \cite{lauria}.Then branches of applications and related works of cryptography with different NN models were proposed subsequently. In particular, it came out network stochastic synchronization provides a possibility for mutual learning between neural networks, allowing researchers to build secure communication channels. \\
In the last few years, main ingredients of neural cryptography have been ported to Quantum Neural Networks (QNN); an interesting example can be found in \cite{china}. QNNs are indeed famous for their advantages in fast learning and improving the efficiency of information processing. \\
In this chapter we discuss some of the fist neural cryptosystems.
\newpage

\section{Neural Networks in Cryptography: an interesting attempt}
This section describes one of the first attempts in designing a neural network to be practically used in both cryptography and cryptanalysis. It must be said that the results attained in \cite{volna}, the paper to which I refer, are still quite rough: synchronization-based approach was not yet developed at the time of this publication. However, this work contains interesting key-concepts of neural cryptography. 

\subsection{Genetic Algorithms in Neural Network Design}
Main element in this research are feedforward neural nets with Back Propagation, but the most interesting characteristic of the approach chosen by Volná is that it relies on EP (Evolutionary Programming): genetic algorithms are used for optimization of the designed NN topology. This is based on a previous work of the same author, that is \cite{volna2}. \\
The criterion of choice is the minimization of the sum of square of deviation of output from neural network. At first, the maximal architecture of the nets is proposed, then, at each step, to optimize the population it is necessary to solve the cryptographic problems of interest. Thereafter the process of genetic algorithms is applied. An optimal population is found either when it achieves the maximal generation or when fitness function achieves the maximal defined value. \\
At this point, it is required to complete the "best" architecture by adapting weights and hence three digits are generated for every
connection coming out from a unit. If the connection does not exist, three zeroes are assigned, else weights are computed this way:
\begin{align}
w_{ij,kl} = \eta[e_2(e_1 2^1 + e_0 2^0)]; \quad i,k\in\{1,\dots,L\}, \\
  j\in\{1,\dots, n_i\}, \nonumber \\
  l \in\{1, \dots, n_k\}, \nonumber 
\end{align}
where $w_{ij,kl} = w(x_{ij}, x_{kl})$ is the weight value between the $j$-th unit in the $i$-th layer and the $l$-th unit in the $k$-th layer and
\begin{align*}
    \eta &= \text{learning parameter;} \quad \eta \in (0,1)\\
    e_0, e_1 &= \text{random digits} \\
  	e_2 &= \text{sign bit}.
\end{align*}
$L$ denotes the total number of layers, while $n_i$ and $n_k$ are the number of units in layers $i$ and $k$, respectively. Recall that these values are determined by the execution of the genetic algorithm. \\
Error between the desired and the real output is then computed and stored in the vector $\vec{E}$ . On the basis of it, the algorithm computes the fitness precursor value $f_i^{\star}$, for each individual $i = 1, \dots, N$, that is
\begin{equation}
f_i^{\star} = k_1(E_i)^2 + k_2(U_i)^2 + k_3(L_i)^2,
\end{equation}
where $k_j$, $j = 1,2,3$ are fixed constants and
\begin{align*}
    E_i &= \text{error for network }i\\
    U_i &= \text{number of hidden units}\\
  	L_i &= \text{number of hidden layers}.
\end{align*}
The general fitness function  $f$ is then calculated as follows:
$$
f_i = \begin{cases}
k - (f_i^{\star} + k_5) \quad \text{if} \quad E_i > k_4 \\
k - f_i^{\star} \quad \text{otherwise.}
\end{cases} 
$$ 
In the above expressions, $k, k_4$ and $k_5$ also denote constants.
The genetic algorithm used by Volná makes use of standard crossover and mutation procedures, as the ones described in the specific chapter. Here we omit details. \\
Adaptation of the best found network architecture is finished with Back Propagation.

\subsection{Volná's experiment}
In this work, the parameters of the adapted neural network become the key of an encryption/decryption algorithm. Topology of such NN clearly depends on the training set that, in the case of Volná, is represented in table ~\ref{table:traingset}, while the chain of chars of the plain text is equivalent to a binary value, that is 96 less than its ASCII code. The cipher text is a randomly generated chain of bits.
\begin{table}[]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{3}{|c|}{\textbf{Plaintext}}                                                                                                                     & \textbf{Cyphertext}                                                          \\ \hline
\textit{Char} & \textit{\begin{tabular}[c]{@{}c@{}}ASCII\\ Code\end{tabular}} & \textit{\begin{tabular}[c]{@{}c@{}}Bit String\\ Representation\end{tabular}} & \textit{\begin{tabular}[c]{@{}c@{}}Bit String\\ Representation\end{tabular}} \\ \hline
a             & 97                                                            & 00001                                                                        & 000010                                                                       \\ \hline
b             & 982                                                           & 00010                                                                        & 100110                                                                       \\ \hline
c             & 99                                                            & 00011                                                                        & 001011                                                                       \\ \hline
d             & 100                                                           & 00100                                                                        & 011010                                                                       \\ \hline
e             & 101                                                           & 00101                                                                        & 100000                                                                       \\ \hline
f             & 102                                                           & 00110                                                                        & 001110                                                                       \\ \hline
g             & 103                                                           & 00111                                                                        & 100101                                                                       \\ \hline
h             & 104                                                           & 01000                                                                        & 010010                                                                       \\ \hline
i             & 105                                                           & 01001                                                                        & 001000                                                                       \\ \hline
j             & 106                                                           & 01010                                                                        & 011110                                                                       \\ \hline
k             & 107                                                           & 01011                                                                        & 001001                                                                       \\ \hline
l             & 108                                                           & 01100                                                                        & 010110                                                                       \\ \hline
m             & 109                                                           & 01101                                                                        & 011000                                                                       \\ \hline
n             & 110                                                           & 01110                                                                        & 011100                                                                       \\ \hline
o             & 111                                                           & 01111                                                                        & 101000                                                                       \\ \hline
p             & 112                                                           & 10000                                                                        & 001010                                                                       \\ \hline
q             & 113                                                           & 10001                                                                        & 010011                                                                       \\ \hline
r             & 114                                                           & 10010                                                                        & 010111                                                                       \\ \hline
s             & 115                                                           & 10011                                                                        & 100111                                                                       \\ \hline
t             & 116                                                           & 10100                                                                        & 001111                                                                       \\ \hline
u             & 117                                                           & 10101                                                                        & 010100                                                                       \\ \hline
v             & 118                                                           & 10110                                                                        & 001100                                                                       \\ \hline
w             & 119                                                           & 10111                                                                        & 100100                                                                       \\ \hline
x             & 120                                                           & 11000                                                                        & 011011                                                                       \\ \hline
y             & 121                                                           & 11001                                                                        & 010001                                                                       \\ \hline
z             & 122                                                           & 11010                                                                        & 001101                                                                       \\ \hline
\end{tabular}
\caption{The training set.}
\label{table:traingset}
\end{table}
Thus, the decrypting neural network has six input units and five output ones, with an unspecified number of hidden units. Viceversa, the net that performs encryption has five input neurons and six output ones. \\
This encryption scheme is symmetric: it uses a single key for both encryption and decryption. It is interesting to notice that Volná, in his publication, thought that this feature was very bad for his encryption system, due to the popularity and goodness of asymmetric, non-neural cryptography. In fact, this model has many limits, but we'll see in next chapters that most modern (and secure) neurocryptographic protocols still are symmetric. Leaving aside asymmetric protocols is indeed one of the main strengths of this new approach to cryptography. \\
Going back to the protocol, the key will include the adapted neural network parameters; that is its topology (architecture) and its configuration (the weight values on connections). Uniquely identifying the NN is hence equivalent to uniquely characterizing the encryption/decryption function.

\newpage

\section{The KKK Key Exchange Protocol}
We now deal with the first complete cryptosystem based on neural networks. It has been later referred to as \textit{KKK}, from the surnames of its inventors. It first appeared in \cite{kanter}, a year later than the work of Volná. \\
Here, a new concept appears in neurocryptography: synchronization of two nets to build a secure communication channel.

\subsection{The Protocol}
Object of the aforementioned work is a key-exchange protocol based on a learning process of feedforward neural networks. The two NNs participating in the communication start from private key vectors $E_k(0)$ and $D_k(0)$. Mutual learning from the exchange of public information leads the two nets to develop a common, time dependent key: $E_k(t) = - D_k(t)$. This is then used for both encryption and decryption. \\
\begin{figure}[]
\centering
\includegraphics[width=0.9\textwidth]{pictures/tpm.jpg}
\caption{An example of tree parity machine.}
\label{fig:tpm}
\end{figure}
This phenomenon, known as synchronization of synaptic weights, has the core feature of speed. In fact, experiments of the authors show that such synchronizing is faster than the process of tracking the weights of one of the networks by an eavesdropper. It must be said that the inventors were not able to find a mathematical proof of this, that instead was published little later by other cryptographers in \cite{shamir}. In this same work, all of the limitations of \textit{KKK} are also shown, but we will deal with this in the following subsections (Synchronization: \ref{ssc:synchro}; Attacks: \ref{ssc:atk}).  \\
Going back to the model, the architecture used by both sender and recipient is a tree parity machine, a two-layered perceptron with $K$ hidden units, $K \times N$ input neurons and a single output. Input units take binary values $x_{kj} = \pm 1$, $k = 1,\dots, K$ and $j = 1, \dots, N$. The $K$ binary hidden units are denoted by $y_k$, $k = 1,\dots, K$, while the integer weight from the $j$-th input unit from the $k$-th hidden unit is denoted $w_{kj}\in\{-L,\dots,L\}$. Output $O$ is the product of the state of the hidden neurons. A graphical representation of a tree parity machine is reported in figure \ref{fig:tpm}. \\
Fix for simplicity $K = 3$ and let $w_{kj}^S$, $w_{kj}^R$ be the secret information of sender and recipient, respectively (that is, the initial values for the weights). Hence this consists of $3N$ integer numbers for each of the two participants.  Note that, from now on, superscript $S$ will denote sender, while $R$ will denote recipient. \\
Each network is then trained with the output of its partner. At each step, both for synchronization and for encryption/decryption steps, a new common public input vector is needed. Given $\vec{x}_{kj}$, output is computed in two steps. In the first one, states of hidden units are computed as 
\begin{equation}
y_k^{S/R} = \sign\left(\sum_{j=1}^{N}w_{kj}^{S/R}x_{kj}\right),
\end{equation}
with the convention $y_k^S = 1$ and $y_k^R = -1$ whenever argument of the sign function is zero. In second step, output is computed as the product of the hidden units:
\begin{equation}
O^{S/R} = \prod_{k=1}^{3}y_k^{S/R}.
\end{equation} 
Sender and recipient send their outputs to each other and in case they do not agree on them (if $O^SO^R<0$), weight are updated according to the following Hebbian rule:
\begin{align}
\text{if} \quad \left(O^{S/R}y_k^{S/R}>0\right) \quad &\text{then} \quad w_{kj}^{S/R} \leftarrow w_{kj}^{S/R} -O^{S/R} x_{kj}; \nonumber \\
\text{if} \quad \left(|w_{kj}^{S/R}|>L\right) \quad &\text{then} \quad w_{kj}^{S/R} \leftarrow \sign\left(w_{kj}^{S/R}\right)L.
\label{hebblearn}
\end{align}
Note that this algorithm only updates weights belonging to the hidden units which are in the same state as that of their output unit. \\
Synchronizing time depends on the choice of the parameters, but this learning rule implies that, as soon as the two networks are synchronized, so they stay forever. Moreover, it was already clear for these experimenter that those times were relatively short compared to the task of externally intercepting weights of one of the two participants (using the same net structure). \\
As soon as the weights are antiparallel, the initialization of the cryptosystem is completed and the secure communication may start. Here we have two possibilities: either use a conventional algorithm, for example a stream cipher, or use the parity machine itself. In the first case, we can build the seed for a pseudo-random number generator basing on the weight vector after synchronization. In the other case, we directly use the output bit of the net for a stream cipher. Note that, using this approach, the complexity of encryption/decryption is linear. 

\subsection{More details on Synchronization}
\label{ssc:synchro}
In the work by Kanter, many experiments are reported to show the effectiveness of synchronization process. However, I decided to omit these details since, as I anticipated in previous section, the "sacred cow" of cryptography Adi Shamir provided us with a mathematical proof of it. \\
The full treatment of this problem found in \cite{shamir} relies on the properties of random walks in bounded domains. Another assumption: learning rule \eqref{hebblearn} would complicate the notation, as it forces the mutually learning NN's into anti-parallel states. Instead, a dual scheme in which the two parties eventually become identical is proposed. Given input vectors $\vec{x}_{kj}$, hidden outputs are calculated as
\begin{equation}
y_k^{S/R} = \sign\left(\sum_{j=1}^{N}w_{kj}^{S/R}x_{kj}\right),
\end{equation}
but with the standard convention 
\begin{equation}
\sign(x) = \begin{cases}
1 \qquad \text{if} \quad x \geq 0 \\
-1 \quad \text{otherwise.}
\end{cases}
\end{equation}
Final output is computed in the same way as before, that is $O^{S/R} = \prod_{k=1}^{3}y_k^{S/R}$. In this version of the algorithm, after the output exchange, each party updates its weights only if $O^S=O^R=O$ and in this case 
\begin{equation}
\label{shamirlearn}
\text{if} \left(y_k^{S/R}=O\right) \quad \text{then} \quad 
w_{kj}^{S/R} \leftarrow B_{-L,L}\left(w_{kj}^{S/R} - y_k^{S/R}x_{kj}\right),
\end{equation}
where
\begin{equation}
\label{rebound}
B_{-L,L}(w) = \begin{cases}
w \qquad \text{if} \quad |w|<L \\
L \qquad \text{if} \quad w>L \\
-L \qquad \text{otherwise}.
\end{cases}
\end{equation}
In other words, above function rescales weights to the prefixed bounds whenever any of them exceeds the allowed values. \\
After these premises, we can show how the two participants converge and why a third net cannot converge to the same parameters by following the same learning procedure. \\
 We start off by considering an oversimplified model of a single perceptron with a single weight. \iftrue Let $w^{S/R}_{t}$  be the current weights and denote inputs $x_{t} \in \{-1,1\} $. \fi The update process of each weight can be described as a random walk with absorbing boundaries $-L,L$, starting from a random point $w^{S/R}_{0}\in\{-L,\dots ,L\}$. \\
At each round $t$, sender and receiver decide either to move their respective weights in the same direction (determined by $x_t$), or not to vary them. In first case, if any of $w^{S/R}_t$ tries to step beyond the fixed boundaries, it remains stuck at it (due to \eqref{rebound}), while the other one gets nearer.  If none of the the weights is absorbed, we have $|w_{t+1}^S - w_{t+1}^R| = |w_{t}^S - w_{t}^R|$, else their distance is reduced by one. Since a random walk is expected to hit its boundaries infinitely often, the paths of the weights admit a mixing time. \\
A simple generalization: consider the case of a single perceptron with multiple weights. Here, the two weight vectors move in the same direction determined by inputs in a multidimensional rectangle, and along each coordinate the distance is either preserved or reduced by one. When all these distances are reduced to zero, the two random walks mix. \\
The general case of neural networks with multiple perceptrons is more complicated, since the two parties may update different subsets of their perceptrons in each round. The complete treatment of such problem can be found in \cite{shamir} but I decided to omit these details.


\subsection{Attacking KKK}
\label{ssc:atk}
The creators of \textit{KKK} claimed its security basing on their experiments. In fact, simulations shown clearly that it was computationally unfeasible for an attacker to intercept any of the parameters related to the two parties, if such eventual attacker relied on the same neural network structure as the ones used in communication. \\
In the paper by Shamir there is also a mathematical proof of this impossibility, but most interesting content of such work is in its last section. This is dedicated to the cryptanalysis of \textit{KKK}, which can indeed be broken by using other types of attack. \\
For example, many successful cryptanalytic applications of Genetic Algorithms can be found in literature. \\
In this case, a large population of NNs with the same structure as the two participants is simulated. These nets are then trained with the same inputs of the two communicant ones. Networks whose outputs mimic those of the two parties breed and multiply, while unsuccessful networks die. \\
Shortly after sender $S$ and recipient $R$ synchronize, they can be sure of this fact (since their outputs keep coinciding) and the same can be checked for any neural network of the attacker. \\
An interesting experimental result: in the majority of tests led by the authors, at least one  of the attacking nets became synchronized with $S$ even before $S$ and $R$ became fully synchronized. \\
Other examples of successful attacks to the \textit{KKK} protocol are the one based on geometrical reasoning and the probabilistic case. The former exploits the geometrical representation of inputs and weight in an algorithm that guesses hidden outputs in the nets of the two parties. The latter, instead, is again based on the properties of generalized random walks: these allow us to compute conditional probabilities for the values of hidden states.

\chapter{Neural Cryptanalysis}
Not only neural networks reveal themselves capable of learning how to communicate securely. In fact, recent studies show that NN's can be employed to perform efficient cryptanalysis over lightweight ciphers. The present chapter is dedicated to discussing an example of such studies. Before this, it is important to recall a previous study of interest, that is \cite{alani}. In this work, Neural Networks, together with the Known Plaintext Attack (KPA) methodology, are efficiently employed to break Data Encryption Standard. A similar combination, that is NN's together with the Chosen Plaintext Attack (CPA), will be later used to build extremely strong cryptosystems (see Subsection \ref{cpaanc}). 

\section{Cryptanalysis of Simon Cipher using Neural Networks}
The paper I am considering in this section is \cite{jay}, in which the author describes a novel, complete approach of using neural networks in cryptanalysis. The cryptosystem of interest belongs to the recent family of Simon block ciphers, publicly released by NSA in \cite{nsa}. Such publication also introduces the family of Speck block ciphers, that is the sister algorithm of Simon. Essentially, Simon has been optimized for performance in hardware implementations, while Speck has been optimized for software implementations.
%(Add citation https://eprint.iacr.org/2013/404.pdf). \\
\subsection{Simon Cipher}
Simon cipher can be consider a lightweight cipher; as such it generally finds its use in devices that have very restricted hardware resources. Hence we remark it cannot be considered a top-security algorithm, but it is good for research purpose; that is, experimenting the capabilities of neural networks in cryptanalysis. \\
Simon has has input block size of length $32$, with a key size of $64$ bits in length. In this study, Simon cipher is round-reduced: it encrypts the plaintexts with first a single round of the Feistel network and then with two rounds; same is for decryption (see figure \ref{fig:simon}).
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{pictures/simon.png}
\caption{Round function of Simon Cipher.}
\label{fig:simon}
\end{figure}
More precisely, the $32$ bits long plaintext $x$ is divided into two blocks of $16$ bits in length, namely $x_i$ and $x_{i+1}$, respectively. The former half is made to go through several left circular shift units, with $S^j$, $j = 1,2,8$, representing shift by one, two and eight bits respectively. A bitwise \textit{AND} is performed on the results of $S^1$ and $S^8$, followed by a bitwise \textit{XOR} with the second half $x_{i+1}$. The result of previous operation is again \textit{XOR}-ed with the result from $S^2$. \\
In general, Simon creates a list of key words $k_0, k_1, \dots, k_T$, where $T$ is the number of rounds. Since in this case the algorithms is simplified to one round and two rounds of the cipher, the key schedules generate only one and two key words respectively. The corresponding key word from the key schedule is then \textit{XOR}-ed with the result of the last performed operation. In the final step of the round, the result from the last step is swapped with the input block $x_{i+1}$ and then passed on to the next round.

\subsection{Neural Network Design}
The architecture of choice is the multi-layer perceptron, with some extensions that we discuss in the following. \\
Each neuron in the input layer corresponds to each bit of the plaintext and ciphertext pairs; that is $64$ input neurons. Same size for the output layer, in which each cell predicts whether the key bit is $0$ or $1$. The number of neurons in hidden layers, instead, varies up to $1024$. According to the author of \cite{jay}, this decision helped increase the accuracy of the model. For the same reason, all layers are fully-connected (FC).\\
A few words about the activation function. It takes into account a bias assigned to the neuron and it's known in literature as Linear Rectifier, that is 
\begin{equation}
y = f(z)= z + b = \sum_j w_jx_j + b,
\end{equation}
where the $x_j$'s are the inputs with the corresponding weights $w_j$; and $b$ denotes the bias. Weights of the neurons are initialized randomly according to a uniform distribution over the interval $(0,1)$ and are updated after each iteration; same for the biases.

\subsection{Data and Results}
In the experiment described in \cite{jay}, training data consists in a large set of records, each of which is a $32$-bit, randomly generated plaintext, with the corresponding $32$-bit long ciphertext. An example can be seen in table \ref{trainingsim}. Around 5 million records were generated with 1000 keys (where each key encrypted 5000 plaintexts). \\
The neural nets describes in previous section are implemented using \textit{Keras}, configured to work on top of \textit{Tensorflow}. The project required the installation of the GPU versions of such softwares. In fact, not being run on a Cloud, the training of such networks clearly appears to be computationally expensive. 

\begin{table}[h!]
\centering 
\tiny
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Plaintext}} & \multicolumn{1}{c|}{\textbf{Cyphertext}} & \multicolumn{1}{c|}{\textbf{Key}}                                                                           \\ \hline
11110010001101100000010100000001         & 01010001000111101111100000010010         & \begin{tabular}[c]{@{}l@{}}11000000001010010011101101111101\\ 11011010011111110100010011101000\end{tabular} \\ \hline
01100101011100100110100001011011         & 11000001010110010111111001001101         & \begin{tabular}[c]{@{}l@{}}11101110100100011010000001011000\\ 10111011001101100101001101101011\end{tabular} \\ \hline
00000101110101101011011001000011         & 10101110001000010110100110000001         & \begin{tabular}[c]{@{}l@{}}01101110000101111011000010011110\\ 10011110110010000001101100001010\end{tabular} \\ \hline
01010001000100001011010000101101         & 11001100101010011010110001100001         & \begin{tabular}[c]{@{}l@{}}00011101101000001011010001111110\\ 01100111001101110110110001000001\end{tabular} \\ \hline
00110101101100100001111101001100         & 11010010110010110010110101010011         & \begin{tabular}[c]{@{}l@{}}01001000100110100010110010001011\\ 01010100000001111101111001101110\end{tabular} \\ \hline
\end{tabular}
\caption{Sample training records.}
\label{trainingsim}
\end{table}


The trained neural network model is then used to predict the key of the plaintext-ciphertext pairs. The accuracy of the network is measured by identifying the number of bits predicted by the neural network that are the same as the key bits in the original key. Better results were attained when considering a single round of the Speck cipher. In this case, after a considerable number of training epochs, an accuracy around 70\% was reached. \\
A powerful extension of this project could be a design based on fuzzy classifiers that could be used along with the neural networks to yield probabilities for zeros and ones, rather than hard predictions that this model does. 
\newpage
\section{Cryptanalysis of Speck Cipher using Neural Networks}
Another sensible improvement in using NN's to attack these block ciphers can be found in \cite{gohr}. In this paper, efficients attack to Speck (the software version of Simon) is implemented by making use of convolutional neural networks. In this case, the attack reveals to be effective even against ciphertext created with up to eleven rounds of the cipher.

\subsection{Speck Cipher}
As we mentioned before, Speck is an iterated block cipher firstly appeared in \cite{nsa} with the aim of building a cipher efficient in software implementations in IoT (Internet of Things) devices. Just like its sister algorithm Simon, it is a composition of the basic functions of modular addition (mod $2^k$), bitwise rotation and bitwise addition applied to $k$-bit strings. More precisely, denote by $\oplus$ bitwise addition, and modular addition modulo $2^k$ by $\boxplus$. Moreover let $\ll$ and $\gg$ denote bitwise rotation of a fixed-size word to the left and to the right, respectively.  \\
The round function of Speck,
\begin{equation}
F: \mathbb{F}_2^k \times \mathbb{F}_2^{2k} \longrightarrow \mathbb{F}_2^{2k},
\end{equation}
takes as input a $k$-bit subkey $K$ and a cipher state consisting of two $k$-bit words $(L_i,R_i)$ and produces from this the next round state $(L_{i+1}, R_{i+1})$. With the above conventions, such step can be summarized as follows:
\begin{align}
L_{i+1} &= \left((L_i \gg \alpha) \boxplus R_i \right) \oplus K, \\
R_{i+1} &= \left(R_i \ll \beta \right) \oplus L_{i+1}.
\end{align}
Here $\alpha$ and $\beta$ are constants specific to each member of the Speck cipher family. The round function is applied a fixed number of times. For example, in the original version of Speck32/64, 22 rounds are performed.

\subsection{Differential Attacks}
Let us first introduce some notations and conventions related to (neural) differential attacks. In these type of attacks, neural networks are essentially trained to distinguish the output of Speck with a given input difference from random data. In \cite{gohr}, differential cryptanalysis means cryptanalysis with regards to bitwise differences in the adversary-controlled input to the cipher under study (and here is why this can be considered a Chosen-Plaintext Attack). \\
Let $F: \{0,1\}^n \longrightarrow \{0,1\}^m$ be a map. Then, a differential transition for $F$ is a pair
\begin{equation}
\left(\Delta_{\text{in}},\Delta_{\text{out}} \right) \in \{0,1\}^n \times \{0,1\}^m.
\end{equation}
The probability $\mathbb{P}\left( \Delta_{\text{in}} \rightarrow \Delta_{\text{out}}\right)$ of the differential transition $F:\Delta_{\text{in}} \mapsto \Delta_{\text{out}} $ is defined as
\begin{equation}
\mathbb{P}\left( \Delta_{\text{in}} \rightarrow \Delta_{\text{out}}\right) =
\frac{\#\left(\{x \in \{0,1\}^n: F(x) \oplus F(x \oplus \Delta_{\text{in}} ) = \Delta_{\text{out}} \} \right)}{2^n},
\end{equation}
where $\#(\mathcal{S})$ denotes the cardinality of set $\mathcal{S}$. For a cipher iteratively constructed by repeated application of a round function, a differential trail is a sequence of differential transitions, given by a sequence of differences $\Delta_0, \Delta_1, \dots, \Delta_n$. \\
In this setting, a differential attack is any attack that uses nonrandom properties of the output of a cryptographic primitive when it is being given input data with a known difference distribution. In particular, multiple differential attacks are attacks where information from an arbitrary set of differential transitions is exploited in order to maximize the gain of the resulting attack. \\
A distinguisher is a classifier $\mathcal{C}$ that accepts as input $d$ data sampled independently from a finite sample space $\Omega$ according to one of $n$ probability distributions $\mathcal{D}_i$, $i \in \{1,\dots,n\}$, and outputs a guess of $i$ (for the submitted input item $d$). \\
Multiple differential attacks build cryptographic distinguishers by using a set $\mathcal{S}$ of differential transitions for some cryptographic function $F$ to characterise its behavior. The basic idea is that each transition 
\begin{equation}
\{\Delta_i \mapsto \delta_j\} \in \mathcal{S}
\end{equation} 
has associated with it a probability $p_{ij}$ of being observed given the experimental setting the cipher is being studied in and another probability $\overline{p}_{ij}$ given the opposite situation. Given some observed data $\mathcal{O}$ from the experiment, Bayesian inference can then be used to determine the origin of the data. Further information about such inference procedures and algorithms can be found in the original publication.
\subsection{Neural Distinguishers for Speck}
Main innovation in the work of Gohr consists in using convolutional neural networks to develop distinguish attacks to be used against Speck. Here we report a few details about network structure, training and obtained results. \\
Input layer of the NN consist of $64$ units arranged in a $4\times16$ array. In fact, a pair $(C_0,C_1)$ of ciphertexts can be written as a sequence of four sixteen-bit words $(w_0,w_1,w_2,w_3)$, mirroring the word-oriented structure of Speck. \\
Above layer is followed by a residual tower of two-layer convolutional neural networks, preceded by a single bit-sliced convolution and followed by a densely connected prediction head. Depth of the residual towers varies according to the number of rounds fixed for the cipher, while the prediction head consists of two hidden layers and one output unit, using sigmoid activation. \\
Training and validation data was obtained by generating uniformly distributed keys $K_i$ and plaintext pairs $P_i$, with a fixed input difference $\Delta$, as well as a vector of binary-valued labels $Y_i$. To produce training or validation data for $k$-round Speck, pair $P_i$ was then encrypted for $k$ rounds if $Y_i$ was set, while otherwise the second plaintext of the pair was replaced with a freshly generated random plaintext. \\
Training was run on large datasets consisting in $10^7$ samples each; the author however reported relatively short training times. With this procedure, an averall performance of around $55\%$ for the classifier is reported. This result is indeed quite poor; Gohr hence improved the algorithm using an approach called "Key Ranking". With this extension, that is not reported in the present work, accuracy is increased up to $92\%$. \\
In \cite{gohr} a partial key-recovery attack is also proposed, by extending the distinguisher discussed in the present section. 


\chapter{Adversarial Neural Cryptography}
After a long period without any substantial contribution to neural cryptography, a new, revolutionary approach made its appearance in 2016, thanks to Google's researchers. That is, neural networks can learn to protect the secrecy of their data from other neural networks: they discover forms of encryption and decryption, without being taught specific algorithms for these purposes. This new approach to cryptography has been named ANC (Adversarial Neural Cryptography).

\section{Google's Model}
The research I am considering is \cite{google}. In this paper, a new cryptosystem is proposed. At its core there are adversarial neural networks and in this section I am examining this protocol.

\subsection{Cryptosystem Organization}
Let us start with some notation. In this scenario, we consider the problem of the secure communication between Alice and Bob, denote them $\mathcal{A}$ and 
$\mathcal{B}$, respectively. Consider also a third participant, Eve ($\mathcal{E})$), who wishes to eavesdrop on their communication. Such adversary is passive, that means it can only intercept communications. A graphical representation of this setting can be seen in figure \ref{fig:anc}. \\
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{pictures/anc.png}
\caption{ANC graphical representation.}
\label{fig:anc}
\end{figure}
In this scenario, Alice wishes to send a private message $P$  ("Plaintext") to Bob. Such message must be regarded as an input to $\mathcal{A}$, together with a key $K$. This key is also known by $\mathcal{B}$, that uses it to decipher the output of $\mathcal{A}$; denote it $C$ ("Ciphertext"). Also Eve knows $C$ and tries to recover $P$ from it, but $\mathcal{E}$ cannot know $K$. Hence let $P_{\mathcal{B}}$ and $P_{\mathcal{E}}$ be the respective outputs. \\
Alice, Bob, and Eve are represented by, guess what, neural networks, based on a "Mix and Trasform" architecture. This structure has a first fully-connected
(FC) layer, where the number of outputs is equal to the number of inputs. $P$ and $K$ are fed into this layer, that enables mixing between the bits of these two vectors. Such layer is followed by a sequence of convolutional layers, the last of which produces an output of a size suitable for a plaintext or ciphertext.  These neural networks have parameters, which we write $\theta_{\mathcal{A}}$, $\theta_{\mathcal{B}}$ and $\theta_{\mathcal{E}}$. Moreover, $P$, $C$, $K$, $P_{\mathcal{B}}$ and $P_{\mathcal{E}}$ are vectors of float numbers; in particular values are allowed to range in $(-1, 1)$. \\
The objective of Eve is to reconstruct $P$, that is, to minimize the distance between $P$ and $P_{\mathcal{E}}$. Alice and Bob want to communicate clearly (to minimize the error between $P$ and $P_{\mathcal{B}}$ ), but also to hide their communication from $\mathcal{E}$. $\mathcal{A}$ and $\mathcal{B}$ are hence trained jointly to communicate securely and to defeat Eve: here lies the biggest innovation in ANC. \\
Since we want Alice and Bob to be trained against the best possible version of Eve, we need to assume a probability distribution on plaintexts and keys. After that, we can rephrase the objectives of the parties in terms of expectation. \\
Denote the outputs of Alice and Bob as $A(\theta_{\mathcal{A}}, P, K)$ and $B(\theta_{\mathcal{B}}, C, K)$, respectively. Similarly, write $E(\theta_{\mathcal{E}}, C)$ for the output of Eve. As anticipated above, we need to choose a distance function on the space of plaintexts. In this model, assuming such $P$'s have length $N$, we take
\begin{equation}
d(P,P') = \sum_{i=0}^{N}|P_i - P'_i|,
\end{equation}
that is, the $L^1$ distance. Note that this choice is not crucial.  After defining an example-specific loss function for Eve,
\begin{equation*}
L_{\mathcal{E}}\left(\theta_{\mathcal{A}}, \theta_{\mathcal{E}}, P, K \right) = d\left(P, E(\theta_{\mathcal{E}}, A(\theta_{\mathcal{A}}, P, K) ) \right),
\end{equation*}
we can define the "true" loss function for $\mathcal{E}$ by taking expectations over the distributions of the $P$'s and $K$'s:
\begin{equation}
\label{eveloss}
L_{\mathcal{E}}\left(\theta_{\mathcal{A}}, \theta_{\mathcal{E}}\right) = \mathbb{E}_{P,K} \left[ d\left(P, E(\theta_{\mathcal{E}}, A(\theta_{\mathcal{A}}, P, K) ) \right)\right].
\end{equation}
The optimal configuration for Eve, $O_{\mathcal{E}}$, can hence be found by minimizing the above loss (given a configuration of $\mathcal{A}$) .
\begin{equation}
O_{\mathcal{E}}\left(\theta_{\mathcal{A}} \right) = \textrm{argmin}_{\theta_{\mathcal{E}}}L_{\mathcal{E}}\left(\theta_{\mathcal{A}}, \theta_{\mathcal{E}}\right)
\end{equation}
Here $\textrm{argmin}_{\theta_{\mathcal{E}}}$ indicates the parameter set $\theta_{\mathcal{E}}$ that minimizes the loss of $\mathcal{E}$, as defined in \eqref{eveloss}.\\
Similarly to the case of Eve, a per-example loss for Bob is defined and then extended to the distributions of plaintexts and keys:
\begin{align}
L_{\mathcal{B}}\left(\theta_{\mathcal{A}}, \theta_{\mathcal{B}}, P, K \right) = d\left(P, B(\theta_{\mathcal{B}}, A(\theta_{\mathcal{A}}, P, K), K ) \right) \nonumber \\ 
L_{\mathcal{B}}\left(\theta_{\mathcal{A}}, \theta_{\mathcal{B}}\right) = \mathbb{E}_{P,K}\left[ d\left(P, B(\theta_{\mathcal{B}}, A(\theta_{\mathcal{A}}, P, K), K ) \right)\right].
\end{align}
We now need to define a joint loss function for Alice and Bob, which takes into account the loss of $\mathcal{B}$ and the optimal value of loss of Eve (by maximizing reconstruction error of $\mathcal{E}$). This is: 
\begin{equation}
L_{\mathcal{A}\mathcal{B}}(\theta_{\mathcal{A}}, \theta_{\mathcal{B}}) = L_{\mathcal{B}}\left(\theta_{\mathcal{A}}, \theta_{\mathcal{B}}\right) - L_{\mathcal{E}}(\theta_{\mathcal{A}}, O_{\mathcal{E}}\left(\theta_{\mathcal{A}} \right)).
\end{equation}
Note that also this choice may be subject to variations. Optimal configurations for Alice and Bob can hence by found as 
\begin{equation}
(O_{\mathcal{A}}, O_{\mathcal{B}}) = \textrm{argmin}_{(\theta_{\mathcal{A}}, \theta_{\mathcal{B}})}L_{\mathcal{A}\mathcal{B}}(\theta_{\mathcal{A}}, \theta_{\mathcal{B}}) 
\end{equation}
It must be said that such optimal configurations are not necessarily unique. \\
A few words about training. Such procedure is based upon stochastic gradient descent and on estimated values calculated over examples (not on expected values over a distribution). In addition, $O_{\mathcal{E}}$ is not computed for given values of $\theta_{\mathcal{A}}$ , but simply approximated. This is done by alternating the training of Eve with that of Alice and Bob. \\
Roughly speaking, training proceeds follows. At first, the outputs of $\mathcal{A}$ may be totally incomprehensible for both $\mathcal{B}$ and $\mathcal{E}$. After a few steps, Bob could discover a way to decrypt the messages of Alice at least partially, without Eve being able to do the same. Later, however, $\mathcal{E}$ may start to break this code. Alice and Bob should then find improvements to their security, in a way such that Eve would find impossible to adjust to those ciphers. Note that this kind of alternation is typical in game theory. 

\subsection{ANC and selective protection} 
Authors of ANC described an experiment to study to test this cryptosystem over selective protection: the question of whether neural networks can learn what information to protect. For example, a plaintext may be made up of several components, of which only few of them are required to be kept secret to the adversary. In this case, selective protection would mean a maximization in utility. \\
Consider a dataset consisting of vectors of for values, namely $(A, B, C, D)$. The target is to build and train a system that, given as inputs the first three values, outputs two predictions of $D$:
$\hat{D}$ and $D_{\text{public}}$. The former is most accurate possible estimate, while the latter is defined as the best possible estimate of $D$ that does not reveal any information about the value of $C$. \\
As before, Alice and Bob share a key and both $\mathcal{B}$ and Eve have access to $\mathcal{A}$'s outputs, that are $D_{\text{public}}$ and a ciphertext $T$. Her input is $(A, B, C)$. Bob produces $\hat{D}$, while Eve tries to recover $C$. 
Using strongly correlated values for $(A, B, C)$ and $D$, the authors were able to show that the adversarial training permits approximating $D$ without revealing $C$.
\newpage

\section{Improvements: CPA-ANC}
This section is dedicated to an extension of Abadi and Andersen's model. In fact, also this last model has some limitations, that are shown in \cite{brazilians}. Authors of this paper elaborated an extension to ANC, that is discusses in the following.

\subsection{Chosen-Plaintext Attack ANC}
\label{cpaanc}
Main limitation to the security of original ANC model lies in the job of Eve, that appears to be too hard. It must decrypt a random message having access only to the ciphertext. The consequence is that, under this methodology, Alice and Bob learn to protect against a weak adversary. It is however possible to strengthen Eve by letting it mount a CPA.  \\
Eve chooses two plaintext messages, namely $P_0$ and $P_1$, and sends them to Alice. $\mathcal{A}$ chooses one of the messages randomly, encrypts it to $C$ and sends it to $\mathcal{B}$ and $\mathcal{E}$. The former decrypts the message using a NN, while the latter only outputs $0$ if it believes $P_0$ was encrypted or $1$ if it believes $P_1$ was encrypted. \\
\begin{figure}[h]
\centering
\includegraphics[scale = 0.4]{pictures/cpa-anc.png}
\caption{Chosen-Plaintext Attack ANC.}
\label{fig:cpaanc}
\end{figure}

\subsection{Neural Network Architecture}
A specific network architecture for this model is proposed, based on the continuous extension of logic operator XOR, built to learn a One-Time Pad algorithm (OTP). \\
We recall that the OTP encryption method is a binary additive stream cipher, where generally a stream of (random) keys is generated and then combined with the plaintext for encryption or with the ciphertext for decryption by a XOR addition.\\
It is possible to prove that a OTP encryption scheme is unbreakable if the following conditions are satisfied:
\begin{itemize}
\item the key must be as long as the plaintext;
\item the key must be truly random;
\item the key must be used only once.
\end{itemize}
Going back to the network architecture, consider mapping bit $0$ to angle $0$ and bit $1$ to angle $\pi$ (XOR can be seen as the sum of the angles). Generalizing bits to a continuous space, the following defines the mapping of a bit $b$ into an angle: 
\begin{equation}
\label{eq:bit-to-angle}
f(b) = \text{arccos}(1 - 2b)
\end{equation}
Then consider its inverse:
\begin{equation}
\label{eq:angle-to-bit}
f^{-1}(a) = \frac{1 - \text{cos}(a)}{2}
\end{equation}
The new net, named \textit{CryptoNet}, takes as input $P$ and $K$ and, for each bit, applies \eqref{eq:bit-to-angle}. Next step is a weight matrix (write $W$) multiplication, followed by the inverse transformation \eqref{eq:angle-to-bit}; output is $C$. Note bits $C_i \in (0,1)$. \textit{CryptoNet} will be denote mathematically as the function
\begin{equation}
C = \zeta_n(W, P, K),
\end{equation}
where we recall $n$ is the size of vectors $P$ and $K$.

\subsection{Method}
Let $E_{\mathcal{A}}(\cdot, K)$ be the encryption function of Alice and let $D_{\mathcal{B}}(\cdot, K)$ be the decryption function of Bob. In this setup, these maps are defined to be the same \textit{CryptoNet}:
\begin{equation}
E_{\mathcal{A}}(\cdot, K) = D_{\mathcal{B}}(\cdot, K) = \zeta_n(W, \cdot, K).
\end{equation}
This means, such network must be the inverse of itself. Therefore, to communicate properly, it should be the case that 
\begin{equation}
\zeta_n \left(W,\zeta_n(W,P,K),K \right) = P,
\end{equation}
where we recall that $P$ is the plaintext. \\
The training, similarly to the procedure described in \cite{google}, relies on estimated values calculated alternating the training of Eve with that of Alice and Bob. Note that, unlike the standard training process of neural networks, here we do not have a clear concept of convergence. In fact, when $\mathcal{E}$ changes its network, then the objective functions of $\mathcal{A}$ and $\mathcal{B}$ also change as consequence. Hence, it was necessary to introduce the following two stopping criteria.
\begin{enumerate}
\item If the decryption error of $\mathcal{B}$ is very close to zero and the attacks of $\mathcal{E}$ are as bad as random guesses, then we stop. In this case, training is successful.
\item If the previous stop criterion is not reached in a prefixed number of rounds, training stops. Recall here a round is completed when $\mathcal{A}$ and $\mathcal{B}$ are trained and then $\mathcal{E}$ is trained. If this happens, failure of the training occurs.
\end{enumerate}
With several test and experiments, authors of \cite{brazilians} showed that neural networks in the described setting can learn a perfectly secure cryptosystem, namely OTP.

\begin{appendices}

\chapter{Finite Differences Methods}
\label{fdmappendix}
In numerical analysis, Finite Difference Methods (FDMs) are discretizations used for solving differential equations by approximating them with difference equations that finite-difference approximate the derivatives of interest. In the first chapter of this work, some algorithms for neural network training are presented which rely on the solution of partial differential equations (PDEs).  \\
In the present appendix we discuss the theoretical bases of finite difference methods applied to PDEs. The results presented here are taken from the second chapter of \cite{fdm}, where a much wider treatment of these methods can be found.
\section{An introductory example}
When a finite difference method (FDM) is used to treat numerically a partial differential equation, the differentiable solution is approximated by some grid function, in example, by a function that is defined only at a finite number of grid points that lie in the underlying domain and its boundary. Each derivative that appears in the partial differential equation has to be replaced
by a suitable divided difference of function values at the chosen grid points. Such approximations of derivatives by difference formulas can be generated in various ways; in case they are generated by a Taylor expansions, the approach is generally known in literature as finite difference method. \\
As an introduction to FDMs, consider the following example. We are interested in computing an approximation to a sufficiently
smooth function $u$ that for given $f$ satisfies Poisson’s equation in the unit square and vanishes on its boundary:
\begin{align}
\label{poissonfdm}
-\Delta u = f \quad &\text{in} \quad \Omega := (0,1)^2 \subset \mathbb{R}^2, \nonumber \\
u = 0 \quad &\text{on} \quad \Gamma := \partial \Omega.
\end{align}
FDMs provide values $u_{i,j}$ that approximate the desired function values $U(x_{i,j})$ at the grid points $\{x_{i,j}\}$. Let the grid points in our example be 
\begin{equation}
x_{i,j} = (ih, jh)^{\intercal} \in \mathbb{R}^2, \quad i,j \in \{0,1,\dots, N\}.
\end{equation}
Here $h = \frac{1}{N}$, $N \in \mathbb{N}$, is the mesh size of the grid. \\
At grid points lying on the boundary $\Gamma$ the given function values (which here are homogeneous) can be immediately taken as the point values of the grid functions. All derivatives in problem \eqref{poissonfdm} have however to be approximated by difference quotients. From Taylor's theorem we obtain
\begin{align}
\frac{\partial^2u}{\partial x_1^2}(x_{i,j}) &\approx \frac{1}{h^2}\left(u(x_{i-1,j}) -2u(x_{i,j}) + u(x_{i+1,j}) \right), \\
\frac{\partial^2u}{\partial x_2^2}(x_{i,j}) &\approx \frac{1}{h^2}\left(u(x_{i,j-1}) -2u(x_{i,j}) + u(x_{i,j+1}) \right).
\end{align}
If these formulas are used to replace the partial derivatives at the inner grid points and the given boundary values are taken into account, then an approximate description of the original boundary value problem \eqref{poissonfdm} is given by the
system of linear equations
\begin{align}
&4u_{i,j} - u_{i-1,j} -u_{i+1,j} -u_{i,j-1} -u_{i,j+1} = h^2f(x_{i,j}), \nonumber \\
&u_{0,j} = u_{N,j} = u_{i,0} = u_{i,N} = 0; \quad i,j \in \{1,\dots,N-1\}.
\end{align}
For any $N\in \mathbb{N}$ this linear system has a unique solution $u_{i,j}$ Under certain smoothness assumptions on the desired solution $u$ of the original problem one has $u_{i,j} \approx u(x_{i,j})$.

\section{Method of Finite Differences applied to PDEs: Basic Concepts}
Let us consider the very simple domain $\Omega := (0,1)^n \subset \mathbb{R}^n$. Denote its closure by $\overline{\Omega}$. For the discretization of $\overline{\Omega}$ a set $\overline{\Omega}_h$ of grid points has to be
selected, e.g., we may chose an equidistant grid that is defined by the points
of intersection obtained when one translates the coordinate axes through consecutive equidistant steps with step size $h=\frac{1}{N}$. Here $N\in \mathbb{N}$ denotes the number of shifted grid lines in each coordinate direction. \\
We distinguish between those grid points lying in the domain $\Omega$ and those at the boundary $\Gamma$ by setting
\begin{equation}
\Omega_h := \overline{\Omega}_h \cap \Omega \quad \text{and} \quad \Gamma_h := \overline{\Omega}_h \cap \Gamma.
\end{equation}
Unlike the continuous problem, whose solution $u$ is defined on all of $\overline{\Omega}$, the discretization leads to a discrete solution
\begin{equation}
u_h: \overline{\Omega}_h \longrightarrow \mathbb{R}
\end{equation}
that is defined only at a finite number of grid points. Such mappings are called grid functions. \\
To deal properly with grid functions we introduce the following function spaces:
\begin{align}
U_h &= \{u_h:\overline{\Omega}_h \longrightarrow \mathbb{R} \}, \\
U_h^0 &= \{u_h \in U_h : u_h|_{\Gamma_h} = 0 \}, \\
V_h &= \{v_h: \Omega_h \longrightarrow \mathbb{R} \}.
\end{align}
To shorten the writing of formulas for difference quotients, let us define the following difference operators where the discretization step size is $h > 0$:
\begin{align}
(D_j^+u)(x)&:= \frac{1}{h}\left(u(x +he^j) - u(x) \right); \quad \text{(forward difference quotient)} \nonumber \\
(D_j^-u)(x)&:= \frac{1}{h}\left(u(x) - u(x -he^j) \right); \quad \text{(backward difference quotient)} \nonumber \\
D_j^0 &:= \frac{1}{2}\left(D_j^+ + D_j^- \right); \quad \text{(central difference quotient).} \nonumber
\end{align}
In the above formulas $e^j$ denotes the unit vector in the positive direction of the $j$-th coordinate axis. \\
How can difference approximations be generated? For sufficiently smooth functions $u:\mathbb{R}^n \longrightarrow \mathbb{R}$, by a Taylor expansion one has
\begin{equation}
\label{tsfdm}
u(x + z) = \sum_{k=0}^{m}\frac{1}{k!}\left(\sum_{j=1}^n z_j \frac{\partial}{\partial x_j} \right)^ku(x) + R_m(x,z).
\end{equation}
Here the remainder $R_m(x,z)$ can be written in the Lagrange form, namely
\begin{equation}
R_m(x,z) = \frac{1}{(m+1)!}\left(\sum_{j=1}^n z_j \frac{\partial}{\partial x_j} \right)^{(m+1)} \times u(x + \theta z); \quad \exists \theta = \theta (x,z) \in (0,1).
\end{equation}
If derivatives are replaced by approximating difference formulas that are derived from \eqref{tsfdm}, one can deduce estimates for the consequent error. Further detail can be found in \cite{fdm}. \\
Going back to difference approximation of derivatives, Taylor theorem provides a systematic tool for the generation of such approximations, for example one could show
\begin{equation}
\frac{\partial u}{\partial x_j}(x) = \frac{1}{2h}\left(-3u(x) +4u(x +4u(x +he^j) - u(x +2he^j) \right) + O(h^2).
\end{equation}
In general, any given partial differential equation problem, including its boundary and/or initial conditions, can be expressed as an abstract operator equation
\begin{equation}
Fu = f,
\end{equation}
with appropriately chosen function spaces $U$ and $V$, a mapping $F: U \longrightarrow V$ and $f \in V$. The related discrete problem can be stated analogously as 
\begin{equation}
F_h u_h = f_h,
\end{equation}
with $F_h: U_h \longrightarrow V_h$, $f_h \in V_h$, and discrete spaces $U_h, V_h$.
\end{appendices}


%\selectlanguage{italian}





\begin{thebibliography}{99}

\bibitem[Graupe(2007)]{graupe} {\sc Graupe, D.} (2007). \textit{Principles of Artificial Neural Networks (2nd Edition)}. Word Scientific Publishing, Singapore.
 
\bibitem[McCulloch and Pitts(1943)]{mcculloch} {\sc McCulloch, W.} and {\sc Pitts, W}. (1943). \textit{A logical calculus of the ideas immanent in nervous activity}. Bulletin of Mathematical Biophysics 5, 115-133.

\bibitem[Hebb(1949)]{hebb} {\sc Hebb, D. O.} (1949). \textit{The organization of behavior; a neuropsychological theory}. Wiley, New York.

\bibitem[Rosenblatt(1958)]{rosenblatt} {\sc Rosenblatt, F.} (1958). \textit{The perceptron: A probabilistic model for information storage and organization in the brain}. Psychological Review, 65.

\bibitem[Widrow and Hoff(1960)]{widrow} {\sc Widrow, B.} and {\sc Hoff, M. E.} (1960). \textit{Adaptive Switching Circuits}. IRE WESCON Convention Record, 96-104.
\bibitem[Minsky and Papert(1969)]{minpapert} {\sc MInsky, M.} and {\sc Papert, S. A.} (1969). \textit{Perceptrons: An Introduction to Computational Geometry}. MIT Press.

\bibitem[Rumelhart, Hinton and Williams(1986)]{back} {\sc Rumelhart, D.}, {\sc Hinton, G.} and {\sc Williams, R.} (1986). \textit{Learning representations by back-propagating errors}. Nature 323, 533–536.

\bibitem[Sejnowski and Rosenberg(1987)]{sejnowski} {\sc Sejnowski, T. J.} and {\sc Rosenberg, C. R.} (1987). \textit{Parallel Networks that Learn to Pronounce English Text}. Complex Systems, 1.

\bibitem[Kingma and Ba(2015)]{adam} {\sc Kingma, D.P.} and {\sc Lei Ba, J.} (2015). \textit{Adam: a method for stochatic optimization}. CoRR.
\bibitem[Mitchell(1998)]{mit} {\sc Mitchell, M.} (1998). \textit{An introduction to Genetic Algorithms}. MIT Press.

\bibitem[Holland(1975)]{holland} {\sc Holland, J.} (1975). \textit{Adaptation in Natural and Artificial Systems}. MIT Press. 

\bibitem[Lauria(1990)]{lauria} {\sc Lauria, F. E.} (1990). \textit{On neurocrytology.} Proceedings of the Third Italian Workshop on Parallel Architectures and Neural Networks, 337-343.

\bibitem[Volná(2000)]{volna} {\sc Volnà, E.} (2000). \textit{Using Neural Network in Cryptography}. University of Ostrava.

\bibitem[Volná(1998)]{volna2} {\sc Volnà, E.} (1998).\textit{Learning algorithm which learns both architectures and weights of feedforward neural networks.} Neural Network World. Int. Journal on Neural and Mass-Parallel Compo and Inf. Systems.

\bibitem[Kanter, Kinzel and Kanter(2001)]{kanter} {\sc Kanter, I.}, {\sc Kinzel, W.} and {\sc Kanter, E.} (2001). \textit{Secure exchange of information by synchronization of neural networks}. Bar Ilan University.

\bibitem[Klimov, Mityagin and Shamir(2002)]{shamir} {\sc Klimov, A.}, {\sc Mityagin, A.} and {\sc Shamir, A.} (2002). \textit{Analysis of Neural Cryptography}. Weizmann Institute.


\bibitem[Abadi and Andersen(2016)]{google} {\sc Abadi, M.} and {\sc Andersen, D. G.} (2016). \textit{Learning to protect communications with Adversarial Neural Cryptography}. Google Brain.

\bibitem[Coutinho et al.(2018)]{brazilians} {\sc Coutinho, M.}, {\sc Robson de Oliveira Albuquerque, R.}, {\sc Borges, F. }, {\sc Villalba, L. J. G.} and  {\sc Kim T. H.} (2018). \textit{Learning Perfectly Secure Cryptography to Protect Communications with Adversarial Neural Cryptography}. University of Brasília.

\bibitem[Jayachandiran(2018)]{jay} {\sc Jayachandiran, K.} (2018). \textit{A Machine Learning Approach for Cryptanalysis}. Rochester Institute of Technology.

\bibitem[Beaulieu et al.(2013)]{nsa} {\sc Beaulieu, R.}, {\sc Shors, D.}, {\sc Smith, J.}, {\sc Treatman-Clark, S.}, {\sc Weeks, B.} and {\sc Wingers, L.} (2013). \textit{The Simon and Speck Families of Lightweight Block Ciphers}. National Security Agency.

\bibitem[Alani(2012)]{alani} {\sc Alani, M. M.} (2012). \textit{Neuro-cryptanalysis of DES}. World Congress on Internet Security (WorldCIS-2012)

\bibitem[Gohr(2019)]{gohr} {\sc Gohr, A.} (2019). \textit{Improving Attacks on Round-Reduced Speck32/64 Using Deep Learning}. Bundesamt für Sicherheit in der Informationstechnik (BSI).

\bibitem[Shi et al.(2020)]{china} {\sc Shi, J.}, {\sc Chen, S.}, {\sc Lu, Y.}, {\sc Feng, Y.}, {\sc Shi, R.}, {\sc Yang, Y.} and {\sc Li, J.} (2020). \textit{An Approach to Cryptography Based on Continuous-Variable Quantum Neural Network}. Nature.

\bibitem[Grossmann, Roos and Stynes(2005)]{fdm} {\sc Grossmann, C.}, {\sc Roos H.} and {\sc Stynes, M.} (2005). \textit{Numerical Treatment of Partial Differential Equations (3rd Ed.)}. Springer.
\end{thebibliography}




\end{document}



