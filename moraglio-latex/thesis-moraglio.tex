% !TEX encoding = UTF-8 Unicode
% !TEX TS-program = pdflatex

%%%%%%% La riga soprastante serve per configurare gli editor
%%%%%%% TeXShop, TeXworks e TeXstudio per gestire questo file
%%%%%%% con la codifica UFF-8.
%%%%%%% Se si vuole usare un'altra codifica si veda sotto.
%%%%%%%

%%%%%%%  Esempio con molte opzioni
%%%%%%% Le opzioni nella forma "chiave=valore" sono definite
%%%%%%% perché la classe dalla versione 6.1.00 usa il pacchetto
%%%%%%% xkeyval. Vedere sulla documentazione in inglese o
%%%%%%% in italiano quali chiavi accettano valori.

%%%%%%% L'opzione per il corpo accetta qualsiasi valore, anche fratto
%%%%%%% (per esempio: corpo=11.5pt) e va sempre scritto con una
%%%%%%% unità di misura. L'utente è pregato di non esagerare con
%%%%%%% corpi normali minori di 9.5pt o maggiori di 13pt.
%%%%%%%
%%%%%%% Le opzioni per inputenc e fontenc vanno per prime.
%%%%%%% Vengono ignorate se NON si compone con pdfLaTeX. Ma
%%%%%%% questo è un esempio per pdfLaTeX.
%%%%%%%

 \documentclass[%
    corpo=11pt,
    twoside,
    stile=classica,
    oldstyle,
    autoretitolo,
    tipotesi=magistrale,
    greek,
    evenboxes,
    english
]{toptesi}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Per la codifica d'entrata si può scegliere quella che si vuole,
%%%%%% ma si consiglia di preferire utf8; in ogni caso non scegliere
%%%%%% codifiche specifiche del sistema operativo.

\usepackage[utf8]{inputenc}% codifica d'entrata
\usepackage[T1]{fontenc}%    codifica dei font
\usepackage{lmodern}%        scelta dei font


%\parindent=2mm
%\parskip=1mm

% Vedere la documentazione toptesi-it.pdf per le
% attenzioni che bisogna usare al fine di ottenere un file
% veramente conforme alle norme per l'archiviabilità.


\usepackage{hyperref}

\hypersetup{%
    pdfpagemode={UseOutlines},
    bookmarksopen,
    pdfstartview={FitH},
    colorlinks,
    linkcolor={blue},
    citecolor={blue},
    urlcolor={blue}
  }
%

\usepackage{mathrsfs,amssymb,amsfonts,amsthm,amsmath,amsbsy}
\usepackage{natbib}
\newtheorem{theorem}{Theorem}[section]
%\newenvironment{theorem}{\begin{theorem}}{\end{theorem}}
\newtheorem{proposition}[theorem]{Proposition} 
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma} 
\newtheorem{example}[theorem]{Example} 
\newtheorem{definition1}[theorem]{Definition} \newenvironment{definition}{\begin{definition1}\rm}{\hfill$\square$\end{definition1}}
\newtheorem{remark1}[theorem]{Remark} \newenvironment{remark}{\begin{remark1}\rm}{\hfill$\square$\end{remark1}}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[font=small]{caption}
\usepackage{setspace} %spacing issues
%%%%%%%% Esempio di composizione di tesi di laurea con PDFLATEX
%
%
% Per scrivere testo fasullo in "latinorum"
%\usepackage{lipsum}
%

%%%%%%% Definizioni locali
\newtheorem{osservazione}{Osservazione}% Standard LaTeX


\begin{document}\errorcontextlines=9
%%%%%%% Questi comandi è meglio metterli dentro l'ambiente
%%%%%%% frontespizio o frontespizio*, oppure in un file di
%%%%%%% configurazione personale. Si veda la documentazione
%%%%%%% inglese o italiana.
%%%%%%% Comunque i presenti comandi servono per comporre la
%%%%%%% tesi con i moduli di estensione standard del pacchetto
%%%%%%% TOPtesi.
\selectlanguage{english}

\thispagestyle{empty}

\centerline {\huge{\textsc{UNIVERSITY OF TORINO}}}
\vskip 30 pt

%\centerline {\Large{\textsc DIPARTIMENTO DI MATEMATICA GIUSEPPE PEANO}}
%\vskip 10 pt
%
%\centerline {\Large{\textsc DIPARTIMENTO ESOMAS}}
%\vskip 10 pt
%
%\centerline {\Large{\textsc DIPARTIMENTO DI INFORMATICA}}
%
%\vskip 30 pt
%
%%\centerline {{\textsc SCUOLA DI SCIENZE DELLA NATURA}}
%
%%\vskip 20 pt
%
\centerline {\LARGE{\textsc M.Sc. in Stochastics and Data Science}}
\vskip 30 pt

\centerline {\Large{\textsc Final dissertation}}
\vskip 50 pt





%\begin{tabular}{ccc}
\centerline {\includegraphics[width=4cm]{logounito.pdf}}
%   \end{tabular}

\vskip 1cm

%\centerline {\normalsize {Tesi di Laurea  Magistrale}}

\vskip 0.7cm

\begin{center} %THESIS TITLE
\Large \bf Thesis Title\\

\end{center}

\vskip 1.7cm
\large
\noindent  Supervisor: Elena Cordero  \hfill  {Candidate: Francesco Moraglio}\\
\noindent Co-supervisor: Stefano Barbero




\vskip 2.5cm


\centerline{ACADEMIC YEAR 2019/2020}



%%%%%%% Per cambiare l'offset per la rilegatura; meno offset
%%%%%%% c'e', meglio e'
%\setbindingcorrection{3mm}



\sommario

Insert here a summary of your thesis
% \paginavuota % funziona anche senza specificare l'opzione classica

\ringraziamenti

You can insert here possible thanks and acknowledgements

\tablespagetrue\figurespagetrue % normalmente questa riga non serve ed e' commentata
\indici

%%%%%%%% fine esperimento
\mainmatter

\chapter{Introduction to Neural Networks}
First chapter of this work is dedicated to its main ingredient: Artificial Neural Networks. These are models of computation based loosely on the way in which the brain is believed to work. Since their invention, biologists are interested in using such networks to model biological brains, but most of the impetus comes from their employment in applied sciences: NN's allow us to create machine that can perform "cognitive" tasks.
\section{Biological Perspective}
To start off, an overview on biological neural network is needed, as ANN's in principle attempt to simulate them. An extensive treatment of both principles and applications of neural networks can be found in \cite{graupe}, to which I make reference for this chapter. \\
The biological neural network consists of interconnected nerve cells (called neurons), whose bodies are where most of the neural "computation" takes place. Neural activity passes from one neuron to another through electrical signals which travel from one cell to the following down the neuron's axon, that can be seen as a connection wire. However, the mechanism of signal transmission in not via electrical conduction, but via charge exchange that is transported by diffusion ions. At the terminal end of the axon, a synaptic gap takes place and an electro-chemical process allows communication with the other cell. \\
Since a given neuron may have many synapses, it can connect to many other cells. Similarly, since there are many dendrites (input connections) per each neuron, a single nerve cell can receive signals from many other neurons. It is very important to notice that not all of such interconnections are equally weighted: some have a higher priority than others. Also some are inhibitory and some are excitory. These facts are also key feature of artificial neural networks, as I'll discuss in the following sections.
\section{Principles}
\begin{singlespace}
The theoretical principles of the artificial neural networks first appeared in the pioneering paper \cite{mcculloch}. In this early work, five fundamental assumptions were formulated:
\end{singlespace}
\begin{enumerate}
\item the activity of an artificial neuron is all-or-nothing;
\item a certain fixed number of synapses greater than one must be excited for a neuron to be excited;
\item the only significant delay within the neural system is the synaptic delay;
\item the activity of any inhibitory synapse absolutely prevents the excitation of the neuron at that time;
\item the structure of the interconnection network does not change over time.
\end{enumerate}
Another widely applied principle is the so-called \textit{Hebbian Rule} (or \textit{Hebbian Learning Law}), that was first stated in \cite{hebb} as follows. \\
"When an axon of cell $A$ is near-enough to excite cell B and when it repeatedly and persistently takes part in firing it, then some growth process or metabolic change takes place in one or both these cells such that the efficiency of cell $A$ is increased". \\
Roughly speaking, such rule can be summarized as "cells that fire together wire together", that is the connections between two neurons might be strengthened (in terms of weights) if the neurons fire simultaneously. \\
Notice that above historical principles do not all apply to most modern neural network architectures. Following sections contain the description of the neural network models which have been employed (also in) neurocryptography.
\section{The Perceptron}
\subsection{Basic structure}
\label{percbs}
The first complete neural computation model, that is called Perceptron, first appeared in \citep{rosenblatt} and serves as a building block to most later models. The Perceptron posses the fundamental structure of a neural cell, of several weighted input connections and a single output channel. The input/output relations are defined to be
\begin{align}
z = \sum_{i}w_i x_i \\
y = f_N(z),
\end{align}
where $w_i$ is the weight at the input $x_i$. Such weights, according to the biological model, need to be adjustable. $y$, instead, denotes the cell's output, that is a nonlinear activation function $f_N$ (see Subsection \ref{percact} below) of the node output $z$. \\
It is customary to consider a network of $n$ Perceptrons; in this case, by letting $z_i$ be the summation output of the $i$-th Perceptron and its inputs $x_{i,j}$, the summation relation becomes
\begin{equation}
z_i = \sum_{j=1}^{m}w_{ij} x_{ij}, \quad i \in \{1,\dots,n\},
\end{equation}
or, in vector form 
\begin{equation}
\label{percout}
z_i = \vec{w}_i^{\intercal}\vec{x}_i,
\end{equation}
where
\begin{align*}
\vec{w}_i = \left[w_{i1}, \dots, w_{in} \right]^\intercal \\
\vec{x}_i = \left[x_{i1}, \dots, x_{in} \right]^\intercal,
\end{align*}
for every $i \in \{1,\dots,n\}$. 
\subsection{Activation functions}
\label{percact}
The Perceptron's cell's output differs from the summation output \ref{percout} by the activation operation of the neuron's body, just as the output of the biological cell differs from the weighted sum of its inputs. Such operation is in terms of an activation function $f_N(z_i)$, which is a non-linear operator yielding $i$-th neuron's output $y_i$ to satisfy certain limiting properties. Different functions are in use, but the most common choice is the sigmoid function, that is
\begin{equation}
\label{act-sigmoid}
y_i = f_N(z_i) = \frac{1}{1 + e^{-z_i}}, \quad i \in \{1,\dots,n\}.
\end{equation}
Under this choice, the following relations are satisfied (see figure TO ADD!!).
\begin{align*}
z_i \to - \infty \quad &\Longleftrightarrow \quad y_i \to 0 ;\\
z_i = 0 \quad &\Longleftrightarrow \quad y_i = 0.5; \\
z_i \to  \infty \quad &\Longleftrightarrow \quad y_i \to 1.
\end{align*}
Another popular activation function is
\begin{equation}
\label{act-tanh}
y_i = \frac{1 + \tanh(z_i)}{2} = \frac{1}{1 - e^{-2z_i}},
\end{equation}
whose shape is similar to the one of the previous function. The simplest activation one could choose is the Heaviside step function:
\begin{equation*}
y_i = H(z_i) = \begin{cases}
1 \quad \text{if} \quad z_i \geq 0 \\
0 \quad \text{otherwise.}
\end{cases}
\end{equation*}
Main advantage of the former two over the latter is that they are (mathematically) smooth. \\
In many applications the activation function's output is translated so that it ranges in $\left(-1, 1\right)$, rather than in $\left(0, 1 \right)$. In particular, sigmoid function \eqref{act-sigmoid} becomes 
\begin{equation}
y_i = \frac{2}{1 + e^{-z_i}} -1 = tanh(z_i/2),
\end{equation}
while activation \eqref{act-tanh} is transformed to 
\begin{equation}
y_i = tanh(z_i) = \frac{1 - e^{-2z_i}}{1 + e^{-2z_i}}.
\end{equation}
ADD SHORT DESCRIPTION OF BIAS!
\subsection{Multi-Layer Perceptron}
ADD A FEW WORDS ABOUT MLP, JUST FOR COMPLETENESS

\newpage
\section{Back Propagation Training Algorithm}
Back Propagation (BP) algorithm was first proposed in \cite{back}, as a solution for setting weights (and hence for the training) of multi-layer Perceptrons. 
The availability of a rigorous method to set intermediate weights, namely to train the hidden layers of neural networks, gave a major boost to the further development of such models. \\
In the present section this algorithm is presented in detail. \\
\subsection{The Algorithm}
The BP algorithm starts with computing values of the output layer, which is by definition the only one whose desired outputs are available. Let $\epsilon$ be the the \textit{error-energy} at the output layer, that is
\begin{equation}
\label{errorenergy}
\epsilon = \frac{1}{2}\sum_{k=1}^{N}e_k^2  =\frac{1}{2}\sum_{k=1}^{N}\left(d_k - y_k\right)^2,
\end{equation}
where $N$ is the number of neurons in the output layer. Consider the gradient of $\epsilon$:
\begin{equation}
\nabla \epsilon_k = \frac{\partial \epsilon}{\partial w_{kj}}.
\end{equation}
Recall that $w_{kj}$ is denoting the weight of the $j$-th input to the $k$-th neuron. By the gradient descent procedure (ADD SOMETHING ABOUT THIS?), we have that 
\begin{equation}
w_{kj}(m+1) = w_{kj}(m) + \Delta w_{kj}(m),
\end{equation}
where 
\begin{equation}
\label{deltasub}
\Delta w_{kj} = -\eta \frac{\partial \epsilon}{\partial w_{kj}}
\end{equation}
and $\eta \in \left(0,1 \right)$ is the rate parameter. Note that the minus sign indicates a down-hill direction towards a minimum. \\
With the notations of Subsection \ref{percbs}, we can now substitute 
\begin{equation}
\label{substitution}
\frac{\partial \epsilon}{\partial w_{kj}} = \frac{\partial \epsilon}{\partial z_k} \frac{\partial z_k}{\partial w_{kj}}
\end{equation}
and 
\begin{equation}
\label{graupe6p8}
\frac{\partial z_k}{\partial w_{kj}} = x_j(p) = y_j(p -1),
\end{equation}
with $p$ denoting the output layer. This way, equation \eqref{substitution} becomes 
\begin{equation}
\label{newsubstitution}
\frac{\partial \epsilon}{\partial w_{kj}} = \frac{\partial \epsilon}{\partial z_k}x_j(p) = \frac{\partial \epsilon}{\partial z_k} y_j(p-1).
\end{equation}
We now have to define 
\begin{equation}
\label{phisub}
\phi_k(p) = - \frac{\partial \epsilon}{\partial z_k},
\end{equation}
so that \eqref{newsubstitution} yields 
\begin{equation}
\frac{\partial \epsilon}{\partial w_{kj}} = - \phi_k(p)x_j(p) = -\phi_k(p)y_j(p-1).
\end{equation}
So by this last equation and \eqref{deltasub} we get
\begin{equation}
\label{graupe6p12}
\Delta w_{kj} = \eta \phi_k(p)x_j(p) = \eta \phi_k(p)y_j(p-1).
\end{equation}
Furthermore, by \eqref{phisub}:
\begin{equation}
\label{newphi}
\phi_k = - \frac{\partial \epsilon}{\partial z_k} = - \frac{\partial \epsilon}{\partial y_k} \frac{\partial y_k}{\partial z_k}.
\end{equation}
But, going back to error-energy definition in \eqref{errorenergy}
\begin{equation}
\frac{\partial \epsilon}{\partial y_k} = -\left(d_k - y_k \right) = y_k - d_k,
\end{equation}
where we recall 
\begin{equation}
y_k = f_N(z_k) = \frac{1}{1 + e^{-z_k}}
\end{equation}
and so we have that 
\begin{equation}
\label{graupe6p16}
\frac{\partial y_k}{\partial z_k} = y_k(1 - y_k).
\end{equation}
Consequently, by equations \eqref{newphi} and subsequent ones, 
\begin{equation}
\label{lastphi}
\phi_k = y_k(1 - y_k)(d_k - y_k),
\end{equation}
such that, at the output layer, by \eqref{deltasub} and \eqref{substitution}
\begin{equation}
\Delta w_{kj} = -\eta \frac{\partial \epsilon}{\partial w_{kj}} = -\eta  \frac{\partial \epsilon}{\partial z_k} \frac{\partial z_k}{\partial w_{kj}},
\end{equation}
where, by \eqref{newphi}
\begin{equation}
\Delta w_{kj}(p) = \eta \phi_k(p)y_j(p-1),
\end{equation}
with $\phi_k$ being as in \eqref{lastphi}, to complete the derivation of the setting of output layer's weights. \\
If we now consider, in general, the $i$-th input to the $j$-th neuron of the $r$-th hidden layer, we still have, as before
\begin{equation}
\Delta w_{ji} = -\eta \frac{\partial \epsilon}{\partial w_{ij}},
\quad j \in \{1,\dots,N\}; \quad i \in\{1, \dots, n \}.
\end{equation}
Similarly to \eqref{substitution} it holds
\begin{equation}
\frac{\partial \epsilon}{\partial w_{ji}} = \frac{\partial \epsilon}{\partial z_j} \frac{\partial z_j}{\partial w_{ji}}
\end{equation}
and taking into account equation in \eqref{graupe6p8} and the expression of $\phi$ in \eqref{newphi} we get to
\begin{equation}
\Delta w_{ji} = -\eta \frac{\partial \epsilon}{\partial z_j} y_i (r-1) = \eta \phi_j(r)y_i(r-1).
\end{equation}
This way, again by right-hand side of \eqref{newphi}, we obtain
\begin{equation}
\Delta w_{ji} = -\eta \left[ \frac{\partial \epsilon}{\partial y_j (r)} \frac{\partial y_j}{\partial z_j} \right]y_i(r-1),
\end{equation}
where $\frac{\partial \epsilon}{\partial y_j}$ is inaccessible (just like $\phi_j (r)$ above). \\
Since we're considering backward propagation from the output, error-energy is only determined by upwards neurons:
\begin{equation}
\frac{\partial \epsilon}{\partial y_j (r)} = \sum_k \frac{\partial \epsilon}{\partial z_k (r+1)} \left[\frac{\partial z_k (r+1)}{\partial y_j (r)} \right] = \sum_k \frac{\partial \epsilon}{\partial z_k} \left[ \frac{\partial}{\partial y_j(r)} \sum_m w_{km}(r+1)y_m(r)\right].
\end{equation}
We remark that, in above expression, summation over $k$ is over the neurons of layer $r+1$ that connect to $y_j(r)$, while summation over $m$ is performed over all inputs to each $k$-th neuron of the $(r+1)$-th layer. Keeping in mind the definition of $\phi$, last equation yields
\begin{equation} 
\label{graupe6p25}
\frac{\partial \epsilon}{\partial y_j (r)}  = \sum_k \frac{\partial \epsilon}{\partial z_k (r+1)}w_{kj} = - \sum_k \phi_k(r+1)w_{kj}(r+1),
\end{equation}
since only $w_{kj}(r+1)$ is connected to $y_j(r)$. Consequently, by equations \eqref{newphi}, \eqref{graupe6p16} and \eqref{graupe6p25}
\begin{equation}
\phi_j(r) = \frac{\partial y_j}{\partial z_j}\sum_k \phi_k(r+1)w_{kj}(r +1) = y_j(r)\left[1 -y_j(r)\right]\sum_k \phi_k(r+1)w_{kj}(r +1)
\end{equation}
and finally, with some other substitutions, we obtain
\begin{equation}
\Delta w_{ji}(r) = \eta \phi_j(r)y_i(r-1),
\end{equation}
that is a function of $\phi$ and of the weights of the $(r + 1)$-th layer. \\
Back Propagation algorithm thus propagates up to the first layer ($r = 1$), iterating over a pre-fixed set of training vectors. Weights are initialized randomly, while the learning rate $\eta$ should be adjusted stepwise for faster convergence.
\subsection{Refinements and Extensions of BP}
A series of modifications to the Back Propagation algorithm were introduced by scientists in order to improve the training procedure. Most important extensions include the introduction of bias into neural networks, smoothing the weight adjustment function and rescaling the activation function. \\
If we want to consider bias in a model, we can regard it as a trainable quantity, just as is any other weight. In general, the bias term $b_i$ at the input to the $i$-th neuron is realized in term of a constant input $B$ with its relative weight:
\begin{equation}
b_i = w^b_i B, \quad i \in \{1,\dots,N\}.
\end{equation}
This algorithm sometimes tends to go through instability issues when computing weights and several smoothing techniques were proposed to deal with such problems. For instance, in \cite{sejnowski}, a smoothing term to modify equation \eqref{graupe6p12} is given:
\begin{align}
&\Delta w_{ij}^{(m)} = \alpha \Delta w_{ij}^{(m-1)} + (1-\alpha)\phi_i(r)y_j(r-1)  \nonumber \\
& w_{ij}^{(m+1)} = w_{ij}^{(m)} + \eta \Delta w_{ij}^{(m)},
\end{align}
where $\alpha \in \left(0,1 \right)$.
Last, modifying the range of the sigmoid function, typically to $\left(-0.5,0.5\right)$ seems to improve converge speed of BP algorithm. 


\chapter{Genetic Algorithms: a brief overview}
Genetic Algorithms were invented by John Holland in the 1960s and can be considered a key building block of artificial intelligence. His aims were that of  formally studying the phenomenon of natural adaptation and that of importing such adaptation mechanisms into computer systems. In \cite{holland}, Genetic Algorithms (GA's) are presented as abstractions of biological evolution. Moreover, a theoretical framework for adaptation under GA's is described. \\
Holland's GA is a method for evolving from one population of "chromosomes" to a new population by using an imitation of natural selection together with the genetic-inspired operators of crossover and mutation. In this chapter we briefly discuss how GA's works and why. Main reference for this part is \cite{mit}, whose author was one of Holland's students at MIT.
\section{Definitions and Setting}
The genome of the evolutionary model is represented by a multiset of size $N$:
\begin{equation}
\textbf{P}(t) = \{\vec{v}_1, \dots, \vec{v}_N \},
\end{equation}
whose elements (that are not necessarily distinct) are called chromosomes. Time variable $t\in \mathbb{N}$ denotes the $t$-th generation of the population. Each chromosome $\vec{v}_i$ is a bit vector of fixed length $n$:
\begin{equation}
\vec{v}_i = \left[v_i^{(1)}, \dots, v_i^{(n)} \right], \quad i \in \{1, \dots, N\},
\end{equation}
where $v_i^{(j)} \in \{0,1\}$, $\forall j = 1, \dots, n$. We recall that there exist more advanced ways to represent chromosomes, but here we are limiting to the binary representation for simplicity. \\
We now need to define some probabilities which are involved in the GA. Selection is based on the definition of a fitness function over the population:
\begin{equation}
f: \textbf{P}(t) \longrightarrow \mathbb{R}_0^{+}.
\end{equation}
We're now allowed to define total fitness and average fitness, namely
\begin{align}
F &= \sum_{\vec{v}\in \textbf{P}(t)}f(\vec{v}) \quad \text{and} \\
\overline{F} &= \frac{F}{N}, \label{avgfit}
\end{align}
respectively. We can hence define the selection probability $P_S(\vec{v})$, for each chromosome $\vec{v}$:
\begin{equation}
P_S(\vec{v}) = \frac{f(v)}{F}.
\end{equation}
Note that \eqref{avgfit}, despite not being explicitly computed in the algorithm, has great theoretical relevance (see Subsection \ref{gabehavior}). \\
The other probabilistic quantities involved are crossover probability $P_C$ and mutation probability $P_M$. The former can also be regarded as the fraction of elements of $\textbf{P}(t)$ that will go through reproduction, while the latter can be thought of as the percentage of bits (of all chromosomes) that will change due to random mutations. Both values are fixed and typically we have $P_C >> P_M$.

\section{The Algorithm}
Each execution of the Genetic Algorithm produces an offspring of chromosomes: it moves from  population $\textbf{P}(t)$ to $\textbf{P}(t+1)$. This evolution process happens in three main phases:
\begin{enumerate}
\item selection via \textit{Wheel of Fortune} model;
\item reproduction via crossover;
\item mutation.
\end{enumerate}
\subsection{Selection}
As anticipated just above, in the first phase the algorithm mimics the behavior of wheel of fortune. First, a sequence $\{q_i\}_{i=0}^{N}$ is computed recursively in the following way:
\begin{align}
q_0 &= 0, \nonumber \\
q_1 &= P_S(\vec{v}_1), \nonumber \\
q_2 &= P_S(\vec{v}_1) + P_S(\vec{v}_2), \nonumber \\
&\vdots \nonumber \\
q_i &= \sum_{j=1}^{i}P_S(\vec{v}_j), \nonumber \\
&\vdots \nonumber \\
q_N &= \sum_{j=1}^{N}P_S(\vec{v}_j) = 1.
\end{align}
Such sequence determines a partition of the interval $\left[0,1\right]$ and the algorithm proceeds by "spinning" the wheel of fortune $N$ times . This means, a random number $r\in \left[0,1\right]$ is generated; if $r \leq 1$, then $\vec{v}_1$ is chosen; else if $q_{j-1} < r \leq q_j$ the chromosome of choice is $\vec{v}_j$. Notice that, with this procedure, a single chromosome may be selected several times. 
\subsection{Reproduction}
Second phase simulates biological crossover, thanks to which genetic recombination is guaranteed at every generation. For each chromosome $\vec{v}$ selected in previous phase
\begin{itemize}
\item a random number  $r\in \left[0,1\right]$ is generated;
\item if $r < P_C$, then $\vec{v}$ is chosen for crossover.
\end{itemize}
Chromosomes chosen this way are then randomly coupled. Whenever their number is odd, another random chromosome can be added (or removed from the reproduction subset).\\
For each couple $\vec{v}_A = \left[v_A^{(1)}, \dots, v_A^{(n)} \right] $ and $\vec{v}_B = \left[v_B^{(1)}, \dots, v_B^{(n)} \right] $, a random integer 
$c \in \{1, 2, \dots, n - 1 \}$ is computed and the chromosomes are split in the following way:
\begin{align}
\vec{v}_A &= \left[v_A^{(1)}, \dots, v_A^{(c)} | v_A^{(c+1)}, \dots,  v_A^{(n)} \right]; \nonumber \\
\vec{v}_B &= \left[v_B^{(1)}, \dots, v_B^{(c)} | v_B^{(c+1)}, \dots,  v_B^{(n)} \right].
\end{align}
After that, offspring chromosomes $\vec{w}_A$ and $\vec{w}_B$ are generated by swapping the blocks found above:
\begin{align}
\vec{w}_A &= \left[v_A^{(1)}, \dots, v_A^{(c)} | v_B^{(c+1)}, \dots,  v_B^{(n)} \right]; \nonumber \\
\vec{w}_B &= \left[v_B^{(1)}, \dots, v_B^{(c)} | v_A^{(c+1)}, \dots,  v_A^{(n)} \right].
\end{align}
In other words
\begin{equation}
\vec{w}_A = \begin{cases}
v_A^{(i)} \quad \text{if} \quad 1 \leq i \leq c \\
v_B^{(i)} \quad \text{if} \quad c + 1 \leq i \leq n;
\end{cases} 
\end{equation}
and
\begin{equation}
\vec{w}_B = \begin{cases}
v_B^{(i)} \quad \text{if} \quad 1 \leq i \leq c \\
v_A^{(i)} \quad \text{if} \quad c + 1 \leq i \leq n.
\end{cases}
\end{equation}
\subsection{Mutation}
For each chromosome $\vec{v}_i$ we consider its every bit $v_i^{(j)}$, $\forall j=1,\dots,n$ and the algorithm generates a random $r_j \in \left[0,1\right]$.
If $r_j < P_M$, then the corresponding bit is changed to its two's complement (otherwise it's left unchanged). After these three phases we obtain a new population $\textbf{P}(t+1)$ with 
\begin{equation}
|\textbf{P}(t+1)|=|\textbf{P}(t)|=N.
\end{equation}

\section{Holland Theorem}
Despite GA's are quite simple to describe and implement, understanding their behavior appears to be quite complicated. According to Holland's theory, such algorithms work by discovering, emphasizing and recombining good "building blocks" (taken from chromosomes with high fitness). Such building blocks are formally named "schemas". 
\subsection{Definitions}
A schema is a set of bit strings that can be described as a template made up of the symbols $0$, $1$ and $*$, which denotes a "free entry". For example, the schema
\begin{equation}
\label{schemah}
K = 1**1
\end{equation}
represents the set of all four-bit strings that begin and end with $1$. Vectors belonging to one of such sets are named instances of that schema. \\
For a scheme $H$, we denote $o(H)$ the order of $H$, that is its number of defined bits ($\neq *$) and we let $d(H)$ be the schema's length (the distance between its outermost defined bits). In example before \eqref{schemah}, we have $o(K)=2$ and $d(K)=3$.
\subsection{Behavior of Genetic Algorithms}
\label{gabehavior}
After this few premises, we can discuss how the GA processes schemas. \\
Any given bit string of length $l$ in an instance of $2^l$ different schemas. Thus, given a population of $N$ chromosomes, it contains instances of $k$ different schemes, with $2^l \leq k \leq N2^l$. This means that, at a given generation $t$, while the GA is explicitly evaluating the fitness of the $N$ chromosomes, it is actually implicitly estimating the average fitness of a much larger number of schemas (where the average fitness of a schema is defined to be the average fitness of all possible instances of that schema). Obviously, the estimates of schema average fitness are not calculated or stored by the GA, However, the algorithm's behavior can be described as though it was actually calculating and storing these averages. \\
Let $H$ be a schema with at least one instance present in $\textbf{P}(t)$ and let $m(H,t)$ the number of instances of $H$ at time $t$. Moreover let $\hat{U}(H,t)$ be the observed average fitness. We want to compute 
\begin{equation}
\mathbb{E}\left[m(H, t + 1) \right],
\end{equation}
the expected number of instances of $H$ at time $t+1$. Assume that selection is carried out as described in previous section; the expected number of offspring of a chromosome $\vec{v}$ is equal to
\begin{equation}
\frac{f(\vec{v})}{\overline{F}}.
\end{equation}
We will write $\vec{v} \in H$ to denote "$\vec{v}$ is an instance of $H$". \\
(For now) ignoring the effects of crossover and mutation, we have
\begin{equation}
\label{expectedmult}
\mathbb{E}\left[m(H, t + 1) \right] = \sum_{\vec{v} \in H}\frac{f(\vec{v})}{\overline{F}} = \frac{\hat{U}(H,t)}{\overline{F}}m(H,t)
\end{equation}
by definition, since
\begin{equation}
\hat{U}(H,t) = \frac{\sum_{\vec{v} \in H}f(\vec{v})}{m(H,t)}.
\end{equation}
Thus even though the GA does not compute $\hat{U}(H,t)$ explicitly, the propagation of schema instances in the population depends on this quantity. \\
Crossover and mutation can both destroy and create instances of $H$, but we limit to considering the disruptive effects. We say schema $H$ "survives" under crossover if at least one instance of such schema is present in the next generation. Letting $S_C(H)$ be the probability of such survival, with the customary notations we have
\begin{equation}
S_C(H) \geq 1 - P_C\left(\frac{d(H)}{n-1} \right).
\end{equation}
In short, the probability of survival under crossover is higher for short schemas. \\
The effects of mutations can be quantified as follows. Let $S_M(H)$ be the probability that schema $H$ will survive under mutation. It can be expressed as
\begin{equation}
S_M(H) = (1-P_M)^{o(H)},
\end{equation}
where we recall that $o(H)$ is the number of defined bits in the schema. Holds because mutation acts independently on each bit of each chromosome. In this case, longer schemes appear to be more resistant. \\
These disruptive effects can be used to amend equation \eqref{expectedmult}, in order to get a lower bound for $\mathbb{E}\left[m(H, t + 1) \right]$. Hence we get
\begin{equation}
\mathbb{E}\left[m(H, t + 1) \right] \geq \frac{\hat{U}(H,t)}{\overline{F}}m(H,t)\left( 1 - P_C\left(\frac{d(H)}{n-1} \right)\right)\left[ (1-P_M)^{o(H)}\right].
\end{equation}
Last result is known as Holland Theorem (or Schemata Theorem). It implies that short, high-average-fitness schemas will see their instances increase exponentially over time.

\chapter{Neural Cryptography: History}
\section{Pioneers of Neural Cryptography}
\newpage

\section{Neural Networks in Cryptography: an interesting attempt}
This section describes one of the first attempts in designing a neural network to be practically used in both cryptography and cryptoanalysis. It must be said that the results attained in \cite{volna}, the paper to which I refer, are still quite rough. However, its importance lies in influencing subsequent works. (CITATIONS NEEDED!!!!!).

\subsection{Genetic Algorithms in Neural Network Design}
Main element in this research are feedforward neural nets with Back Propagation, but the most interesting characteristic of Volna's approach is that it relies on EP (Evolutionary Programming): genetic algorithms are used for optimization of the designed NN topology. This is based on a previous work of the same author, that is \cite{volna2}. \\
The criterion of choice is the minimization of the sum of square of deviation of output from neural network. At first, the maximal architecture of the nets is proposed, then, at each step, to optimize the population it is necessary to solve the cryptographic problems of interest. Thereafter the process of genetic algorithms is applied. An optimal population is found either when it achieves the maximal generation or when fitness function achieves the maximal defined value. \\
At this point, it is required to complete the "best" architecture by adapting weights and hence three digits are generated for every
connection coming out from a unit. If the connection does not exist, three zeroes are assigned, else weights are computed this way:
\begin{equation}
w_{ij,kl} = \eta[e_2(e_1 2^1 + e_0 2^0)],
\end{equation}
where $w_{ij,kl} = w(x_{ij}, x_{kl})$ is the weight value between the $j$-th unit in the $i$-th layer and the $l$-th unit in the $k$-th layer and
\begin{align*}
    \eta &= \text{learning parameter;} \quad \eta \in (0,1)\\
    e_i &= \text{random digits} \quad (i=0,1)\\
  	e_2 &= \text{sign bit}.
\end{align*}
Error between the desired and the real output is the computed and stored in the vector $\vec{E}$ . On the basis of it, the algorithm computes the fitness precursor value $f_i^{\star}$, for each individual $i = 1, \dots, N$, that is
\begin{equation}
f_i^{\star} = k_1(E_i)^2 + k_2(U_i)^2 + k_3(L_i)^2,
\end{equation}
where $k_j$, $j = 1,2,3$ are fixed constants and
\begin{align*}
    E_i &= \text{error for network }i\\
    U_i &= \text{number of hidden units}\\
  	L_i &= \text{number of hidden layers}.
\end{align*}
The general fitness function  $f$ is then calculated as follows:
$$
f_i = \begin{cases}
k - (f_i^{\star} + k_5) \quad \text{if} \quad E_i > k_4 \\
k - f_i^{\star} \quad \text{otherwise.}
\end{cases} 
$$ 
In the above expressions, $k, k_4$ and $k_5$ also denote constants.
The genetic algorithm used by Volná makes use of standard crossover and mutation procedures, as the ones described in the specific chapter. Here we omit details. \\
Adaptation of the best found network architecture is finished with Back Propagation.

\subsection{Volná's experiment}
In this work, the parameters of the adapted neural network become the key of an encryption/decryption algorithm. Topology of such NN clearly depends on the training set that, in Volná's case, is represented in table ~\ref{table:traingset}, while the chain of chars of the plain text is equivalent to a binary value, that is 96 less than its ASCII code. The cipher text is a randomly generated chain of bits.
\begin{table}[]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{3}{|c|}{\textbf{Plaintext}}                                                                                                                     & \textbf{Cyphertext}                                                          \\ \hline
\textit{Char} & \textit{\begin{tabular}[c]{@{}c@{}}ASCII\\ Code\end{tabular}} & \textit{\begin{tabular}[c]{@{}c@{}}Bit String\\ Representation\end{tabular}} & \textit{\begin{tabular}[c]{@{}c@{}}Bit String\\ Representation\end{tabular}} \\ \hline
a             & 97                                                            & 00001                                                                        & 000010                                                                       \\ \hline
b             & 982                                                           & 00010                                                                        & 100110                                                                       \\ \hline
c             & 99                                                            & 00011                                                                        & 001011                                                                       \\ \hline
d             & 100                                                           & 00100                                                                        & 011010                                                                       \\ \hline
e             & 101                                                           & 00101                                                                        & 100000                                                                       \\ \hline
f             & 102                                                           & 00110                                                                        & 001110                                                                       \\ \hline
g             & 103                                                           & 00111                                                                        & 100101                                                                       \\ \hline
h             & 104                                                           & 01000                                                                        & 010010                                                                       \\ \hline
i             & 105                                                           & 01001                                                                        & 001000                                                                       \\ \hline
j             & 106                                                           & 01010                                                                        & 011110                                                                       \\ \hline
k             & 107                                                           & 01011                                                                        & 001001                                                                       \\ \hline
l             & 108                                                           & 01100                                                                        & 010110                                                                       \\ \hline
m             & 109                                                           & 01101                                                                        & 011000                                                                       \\ \hline
n             & 110                                                           & 01110                                                                        & 011100                                                                       \\ \hline
o             & 111                                                           & 01111                                                                        & 101000                                                                       \\ \hline
p             & 112                                                           & 10000                                                                        & 001010                                                                       \\ \hline
q             & 113                                                           & 10001                                                                        & 010011                                                                       \\ \hline
r             & 114                                                           & 10010                                                                        & 010111                                                                       \\ \hline
s             & 115                                                           & 10011                                                                        & 100111                                                                       \\ \hline
t             & 116                                                           & 10100                                                                        & 001111                                                                       \\ \hline
u             & 117                                                           & 10101                                                                        & 010100                                                                       \\ \hline
v             & 118                                                           & 10110                                                                        & 001100                                                                       \\ \hline
w             & 119                                                           & 10111                                                                        & 100100                                                                       \\ \hline
x             & 120                                                           & 11000                                                                        & 011011                                                                       \\ \hline
y             & 121                                                           & 11001                                                                        & 010001                                                                       \\ \hline
z             & 122                                                           & 11010                                                                        & 001101                                                                       \\ \hline
\end{tabular}
\caption{The training set.}
\label{table:traingset}
\end{table}
Thus, the decrypting neural network has six input units and five output ones, with an unspecified number of hidden units. Viceversa, the net that performs encryption has five input neurons and six output ones. \\
This encryption scheme is symmetric: it uses a single key for both encryption and decryption. It is interesting to notice that Volná, in his publication, thought that this feature was very bad for his encryption system, due to the popularity and goodness of asymmetric, non-neural cryptography. In fact, this model has many limits, but we'll see in next chapters that most modern (and secure) neurocryptographic protocols still are symmetric. Leaving aside asymmetric protocols is indeed one of the main strengths of this new approach to cryptography. \\
Going back to the protocol, the key will include the adapted neural network parameters; that is its topology (architecture) and its configuration (the weight values on connections). Uniquely identifying the NN is hence equivalent to uniquely characterizing the encryption/decryption function.

\newpage

\section{The KKK Key Exchange Protocol}
We now deal with the first complete cryptosystem based on neural networks. It has been later referred to as \textit{KKK}, from the surnames of its inventors. It first appeared in \cite{kanter}, a year later than Volná's work. \\
Here, a new concept appears in neurocryptography: synchronization of two nets to build a secure communication channel.

\subsection{The Protocol}
Object of the above cited work is a key-exchange protocol based on a learning process of feedforward neural networks. The two NN's participating in the communication start from private key vectors $E_k(0)$ and $D_k(0)$. Mutual learning from the exchange of public information leads the two nets to develop a common, time dependent key: $E_k(t) = - D_k(t)$. This is then used for both encryption and decryption. \\
This phenomenon, known as synchronization of synaptic weights, has the core feature of speed. In fact, experiments of the authors show that such synchronizing is faster than the process of tracking the weights of one of the networks by an eavesdropper. It must be said that the inventors weren't able to find a mathematical proof of this, that instead was published little later by other cryptographers in \cite{shamir}. In this same work, all of the limitations of \textit{KKK} are also shown, but we'll deal with this in the following subsections (Synchronization: \ref{ssc:synchro}; Attacks: \ref{ssc:atk}).  \\
Going back to the model, the architecture used by both sender and recipient is a tree parity machine, a two-layered perceptron with $K$ hidden units, $K \times N$ input neurons and a single output. Input units take binary values $x_{kj} = \pm 1$, $k = 1,\dots, K$ and $j = 1, \dots, N$. The $K$ binary hidden units are denoted by $y_k$, $k = 1,\dots, K$, while the integer weight from the $j$-th input unit from the $k$-th hidden unit is denoted $w_{kj}\in\{-L,\dots,L\}$. Output $O$ is the product of the state of the hidden neurons. (ADD FIGURE!!!) \\
Fix for simplicity $K = 3$ and let $w_{kj}^S$, $w_{kj}^R$ be the secret information of sender and recipient, respectively (that is, the initial values for the weights). Hence this consists of $3N$ integer numbers for each of the two participants. \\
Each network is then trained with the output of its partner. At each step, both for synchronization and for encryption/decryption steps, a new common public input vector is needed. Given $\vec{x_{kj}}$, output is computed in two steps. In the first one, states of hidden units are computed as 
\begin{equation}
y_k^{S/R} = sgn\left(\sum_{j=1}^{N}w_{kj}^{S/R}x_{kj}\right),
\end{equation}
with the convention $y_k^S = 1$ and $y_k^R = -1$ whenever argument of the sign function is zero. In second step, output is computed as the product of the hidden units:
\begin{equation}
O^{S/R} = \prod_{k=1}^{3}y_k^{S/R}.
\end{equation} 
Sender and recipient send their outputs to each other and in case they do not agree on them (if $O^SO^R<0$), weight are updated according to the following Hebbian rule:
\begin{align}
\text{if} \quad \left(O^{S/R}y_k^{S/R}>0\right) \quad &\text{then} \quad w_{kj}^{S/R} \leftarrow w_{kj}^{S/R} -O^{S/R} x_{kj}; \nonumber \\
\text{if} \quad \left(|w_{kj}^{S/R}|>L\right) \quad &\text{then} \quad w_{kj}^{S/R} \leftarrow sgn\left(w_{kj}^{S/R}\right)L.
\label{hebblearn}
\end{align}
Note that this algorithm only updates weights belonging to the hidden units which are in the same state as that of their output unit. \\
Synchronizing time depends on the choice of the parameters, but this learning rule implies that, as soon as the two networks are synchronized, so they stay forever. Moreover, it was already clear for these experimenter that those times were relatively short compared to the task of externally intercepting weights of one of the two participants (using the same net structure). \\
As soon as the weights are antiparallel, the initialization of the cryptosystem is completed and the secure communication may start. Here we have two possibilities: either use a conventional algorithm, for example a stream cipher, or use the parity machine itself. In the first case, we can build the seed for a pseudo-random number generator basing on the weight vector after synchronization. In the other case, we directly use the output bit of the net for a stream cipher. Note that, using this approach, the complexity of encryption/decryption is linear. 

\subsection{More details on Synchronization}
\label{ssc:synchro}
In Kanter's work, many experiments are reported to show the effectiveness of synchronization process. However, I decided to omit these details since, as I anticipated in previous section, the "sacred cow" of cryptography Adi Shamir provided us with a mathematical proof of it. \\
The full treatment of this problem found in \cite{shamir} relies on the properties of random walks in bounded domains. Another assumption: learning rule \ref{hebblearn} would complicate the notation, as it forces the mutually learning NN's into anti-parallel states. Instead, a dual scheme in which the two parties eventually become identical is proposed. Given input vectors $\vec{x_{kj}}$, hidden outputs are calculated as
\begin{equation}
y_k^{S/R} = sgn\left(\sum_{j=1}^{N}w_{kj}^{S/R}x_{kj}\right),
\end{equation}
but with the standard convention 
\begin{equation}
sgn(x) = \begin{cases}
1 \qquad \text{if} \quad x \geq 0 \\
-1 \quad \text{otherwise.}
\end{cases}
\end{equation}
Final output is computed in the same way as before, that is $O^{S/R} = \prod_{k=1}^{3}y_k^{S/R}$. In this version of the algorithm, after the output exchange, each party updates its weights only if $O^S=O^R=O$ and in this case 
\begin{equation}
\label{shamirlearn}
\text{if} \left(y_k^{S/R}=O\right) \quad \text{then} \quad 
w_{kj}^{S/R} \leftarrow B_{-L,L}\left(w_{kj}^{S/R} - y_k^{S/R}x_{kj}\right),
\end{equation}
where
\begin{equation}
\label{rebound}
B_{-L,L}(w) = \begin{cases}
w \qquad \text{if} \quad |w|<L \\
L \qquad \text{if} \quad w>L \\
-L \qquad \text{otherwise}.
\end{cases}
\end{equation}
In other words, above function rescales weights to the prefixed bounds whenever any of them exceeds the allowed values. \\
After these premises, we can show how the two participants converge and why a third net cannot converge to the same parameters by following the same learning procedure. \\
 We start off by considering an oversimplified model of a single Perceptron with a single weight. \iftrue Let $w^{S/R}_{t}$  be the current weights and denote inputs $x_{t} \in \{-1,1\} $. \fi Each weight's update process can be described as a random walk with absorbing boundaries $-L,L$, starting from a random point $w^{S/R}_{0}\in\{-L,\dots ,L\}$. \\
At each round $t$, sender and receiver decide either to move their respective weights in the same direction (determined by $x_t$), or not to vary them. In first case, if any of $w^{S/R}_t$ tries to step beyond the fixed boundaries, it remains stuck at it (due to \ref{rebound}), while the other one gets nearer.  If none of the the weights is absorbed, we have $|w_{t+1}^S - w_{t+1}^R| = |w_{t}^S - w_{t}^R|$, else their distance is reduced by one. Since a random walk is expected to hit its boundaries infinitely often, the weights'paths admit a mixing time. \\
A simple generalization: consider the case of a single Perceptron with multiple weights. Here, the two weight vectors move in the same direction determined by inputs in a multidimensional rectangle, and along each coordinate the distance is either preserved or reduced by one. When all these distances are reduced to zero, the two random walks mix. \\
The general case of neural networks with multiple Perceptrons is more complicated, since the two parties may update different subsets of their perceptrons in each round. The complete treatment of such problem can be found in the publication to which I'm referring, but I decided to omit these details.


\subsection{Attacking KKK}
\label{ssc:atk}
The creators of \textit{KKK} claimed its security basing on their experiments. In fact, simulations shown clearly that it was computationally unfeasible for an attacker to intercept any of the parties' parameters, if such eventual attacker relied on the same neural network structure as the ones used in communication. \\
In Shamir's paper there's also a mathematical proof of this impossibility, but most interesting content of such work is in its last section. This is dedicated to the cryptanalysis of \textit{KKK}, which can indeed be broken by using other types of attack. \\
For example, many successful cryptanalytic applications of Genetic Algorithms can be found in literature. \\
In this case, a large population of NN's with the same structure as the two participants is simulated. These nets are then trained with the same inputs of the two communicant ones. Networks whose outputs mimic those of the two parties breed and multiply, while unsuccessful networks die. \\
Shortly after sender $S$ and recipient $R$ synchronize, they can be certain of this fact (since their outputs keep coinciding) and the same can be checked for any of the attacker's neural networks. \\
An interesting experimental result: in the majority of tests led by the authors, at least one  of the attacking nets became synchronized with $S$ even before $S$ and $R$ became fully synchronized. \\
Other examples of successful attacks to the \textit{KKK} protocol are the one based on geometrical reasoning and the probabilistic case. The former exploits the geometrical representation of inputs and weight in an algorithm that guesses hidden outputs in the two parties'nets. The latter, instead, is again based on the properties of generalized random walks: these allow us to compute conditional probabilities for the values of hidden states.

\chapter{Neural Cryptanalysis}
Not only neural networks reveal themselves capable of learning how to communicate securely. In fact, recent studies show that NN's can be employed to perform efficient cryptanalysis over lightweight ciphers. The present chapter is dedicated to discussing an example of such studies. Before this, it is important to recall a previous study of interest, that is \cite{alani}. In this work, Neural Networks, together with the Known Plaintext Attack (KPA) methodology, are efficiently employed to break Data Encryption Standard. A similar combination, that is NN's together with the Chosen Plaintext Attack (CPA), will be later used to build extremely strong cryptosystems (see \ref{cpaanc}). \\
\section{Cryptanalysis of Simon Cipher using Neural Networks}
The study I'm considering in this section is \cite{jay}, in which its author describes a novel, complete approach of using neural networks in cryptoanalysis. The cryptosystem of interest belongs to the family of Simon block ciphers, publicly released by NSA in 2013. (ADD CIT.)  
%(Add citation https://eprint.iacr.org/2013/404.pdf). \\
\subsection{Simon Cipher}
Simon cipher can be consider a lightweight cipher; as such it generally finds its use in devices that have very restricted hardware resources. Hence we remark it cannot be considered a top-security algorithm, but it is good for the research's purpose; that is, experimenting the capabilities of neural networks in cryptanalysis. \\
Simon has has input block size of length $32$, with a key size of $64$ bits in length. In this study, Simon cipher is round-reduced: it encrypts the plaintexts with first a single round of the Feistel network and then with two rounds; same is for decryption. (ADD PICTURE)\\
More precisely, the $32$ bits long plaintext $x$ is divided into two blocks of $16$ bits in length, namely $x_i$ and $x_{i+1}$, respectively. The former half is made to go through several left circular shift units, with $S^j$, $j = 1,2,8$, representing shift by one, two and eight bits respectively. A bitwise \textit{AND} is performed on the results of $S^1$ and $S^8$, followed by a bitwise \textit{XOR} with the second half $x_{i+1}$. The result of previous operation is again \textit{XOR}-ed with the result from $S^2$. \\
In general, Simon creates a list of key words $k_0, k_1, \dots, k_T$, where $T$ is the number of rounds. Since in this case the algorithms is simplified to one round and two rounds of the cipher, the key schedules generate only one and two key words respectively. The corresponding key word from the key schedule is then \textit{XOR}-ed with the result of the last performed operation. In the final step of the round, the result from the last step is swapped with the input block $x_{i+1}$ and then passed on to the next round.

\subsection{Neural Network Design}
The architecture of choice is the multi-layer perceptron, with some extensions that we discuss in the following. \\
Each neuron in the input layer corresponds to each bit of the plaintext and ciphertext pairs; that is $64$ input neurons. Same size for the output layer, in which each cell predicts whether the key bit is $0$ or $1$. The number of neurons in hidden layers, instead, varies up to $1024$. According to the authors of the aforementioned publication, this decision helped increase the accuracy of the model. For the same reason, all layers are fully-connected (FC).\\
A few words about the activation function. It takes into account a bias assigned to the neuron and it's known in literature as Linear Rectifier, that is 
\begin{equation}
y = f(z)= z + b = \sum_j w_jx_j + b,
\end{equation}
where the $x_j$'s are the inputs with the corresponding weights $w_j$; and $b$ denotes the bias. Weights of the neurons are initialized randomly according to a uniform distribution over the interval $(0,1)$ and are updated after each iteration; same for the biases.

\subsection{Data and Results}
In the experiment described in \cite{jay}, training data consists in a large set of records, each of which is a $32$-bit, randomly generated plaintext, with the corresponding $32$-bit long ciphertext. Around 5 million records were generated with 1000 keys (where each key encrypted 5000 plaintexts). \\
The neural nets describes in previous section are implemented using \textit{Keras}, configured to work on top of \textit{Tensorflow}. The project required the installation of the GPU versions of such softwares. In fact, not being run on a Cloud, the training of such networks clearly appears to be computationally expensive. \\
(ADD TABLE WITH SAMPLE TRAINING DATA). \\

The trained neural network model is then used to predict the key of the plaintext-ciphertext pairs. The accuracy of the network is measured by identifying the number of bits predicted by the neural network that are the same as the key bits in the original key. Better results were attained when considering a single round of the Speck cipher. In this case, after a considerable number of training epochs, an accuracy around 70\% was reached. \\
A powerful extension of this project could be a design based on fuzzy classifiers that could be used along with the neural networks to yield probabilities for zeros and ones, rather than hard predictions that this model does. \\
For example, another sensible improvement in using NN's to attack this kind of ciphers can be found in \citep{gohr}. In this paper, an efficient Chosen-Plaintext-Attack to Speck (the hardware version of Simon) is implemented by making use of convolutional neural networks. In this case, the attack reveals to be effective even against ciphertext created eight or nine rounds of the cipher. 
(ADD MORE DETAILS? GOHR'S WORK IS VERY COMPLICATED).


\chapter{Adversarial Neural Cryptography}
After a long period without any substantial contribution to neural cryptography, a new, revolutionary approach made its appearance in 2016, thanks to Google's researchers. That is, neural networks can learn to protect the secrecy of their data from other neural networks: they discover forms of encryption and decryption, without being taught specific algorithms for these purposes. This new approach to cryptography has been named ANC (Adversarial Neural Cryptography).

\section{Google's Model}
The research I'm considering is \cite{google}. In this paper, a new cryptosystem is proposed. At its core there are adversarial neural networks and in this section I'm examining this protocol.

\subsection{Cryptosystem Organization}
Let's start with some notation. In this scenario, we consider the problem of the secure communication between Alice and Bob, denote them $\mathcal{A}$ and 
$\mathcal{B}$, respectively. Consider also a third participant, Eve ($\mathcal{E})$), who wishes to eavesdrop on their communication. Such adversary is passive, that means it can only intercept communications. \\
In this scenario, Alice wishes to send a private message $P$  ("Plaintext") to Bob. Such message must be regarded as an input to $\mathcal{A}$, together with a key $K$. This key is also known by $\mathcal{B}$, that uses it to decipher $\mathcal{A}$'s output; denote it $C$ ("Ciphertext"). Also Eve knows $C$ and tries to recover $P$ from it, but $E$ cannot know $K$. Hence let $P_{Bob}$ and $P_{Eve}$ be the respective outputs. \\
Alice, Bob, and Eve are represented by, guess what, neural networks, based on a "Mix and Trasform" architecture. This structure has a first fully-connected
(FC) layer, where the number of outputs is equal to the number of inputs. $P$ and $K$ are fed into this layer, that enables mixing between the bits of these two vectors. Such layer is followed by a sequence of convolutional layers, the last of which produces an output of a size suitable for a plaintext or ciphertext.  These neural networks have parameters, which we write $\theta_{\mathcal{A}}$, $\theta_{\mathcal{B}}$ and $\theta_{\mathcal{E}}$. Moreover, $P$, $C$, $K$, $P_{Bob}$ and $P_{Eve}$ are vectors of float numbers; in particular values are allowed to range in $(-1, 1)$. \\
Eve's objective is to reconstruct $P$, that is, to minimize the distance between $P$ and $P_{Eve}$. Alice and Bob want to communicate clearly (to minimize the error between $P$ and $P_{Bob}$ ), but also to hide their communication from $\mathcal{E}$. $\mathcal{A}$ and $\mathcal{B}$ are hence trained jointly to communicate securely and to defeat Eve: here lies the biggest innovation in ANC. \\
Since we want Alice and Bob to be trained against the best possible version of Eve, we need to assume a probability distribution on plaintexts and keys. After that, we can rephrase the parties' objectives in terms of expectation. \\
Denote Alice and Bob's output as $A(\theta_{\mathcal{A}}, P, K)$ and $B(\theta_{\mathcal{B}}, C, K)$, respectively. Similarly, write $E(\theta_{\mathcal{E}}, C)$ for Eve's output. As anticipated above, we need to choose a distance function on the space of plaintexts. In this model, assuming such $P$'s have length $N$, we take
\begin{equation}
d(P,P') = \sum_{i=0}^{N}|P_i - P'_i|,
\end{equation}
that is, the $L^1$ distance. Note that this choice is not crucial.  After defining an example-specific loss function for Eve,
\begin{equation*}
L_{\mathcal{E}}\left(\theta_{\mathcal{A}}, \theta_{\mathcal{E}}, P, K \right) = d\left(P, E(\theta_{\mathcal{E}}, A(\theta_{\mathcal{A}}, P, K) ) \right),
\end{equation*}
we can define the "true" loss function for $\mathcal{E}$ by taking expectations over the distributions of the $P$'s and $K$'s:
\begin{equation}
L_{\mathcal{E}}\left(\theta_{\mathcal{A}}, \theta_{\mathcal{E}}\right) = \mathbb{E}_{P,K} \left[ d\left(P, E(\theta_{\mathcal{E}}, A(\theta_{\mathcal{A}}, P, K) ) \right)\right].
\end{equation}
The optimal configuration for Eve, $O_{\mathcal{E}}$, can hence be found by minimizing the above loss (given a configuration of $\mathcal{A}$) .
\begin{equation}
O_{\mathcal{E}}\left(\theta_{\mathcal{A}} \right) = \textrm{argmin}_{\theta_{\mathcal{E}}}L_{\mathcal{E}}\left(\theta_{\mathcal{A}}, \theta_{\mathcal{E}}\right)
\end{equation}
Similarly, a per-example loss for Bob is defined and then extended to the distributions of plaintexts and keys:
\begin{align}
L_{\mathcal{B}}\left(\theta_{\mathcal{A}}, \theta_{\mathcal{B}}, P, K \right) = d\left(P, B(\theta_{\mathcal{B}}, A(\theta_{\mathcal{A}}, P, K), K ) \right) \nonumber \\ 
L_{\mathcal{B}}\left(\theta_{\mathcal{A}}, \theta_{\mathcal{B}}\right) = \mathbb{E}_{P,K}\left[ d\left(P, B(\theta_{\mathcal{B}}, A(\theta_{\mathcal{A}}, P, K), K ) \right)\right].
\end{align}
We now need to define a joint loss function for Alice and Bob, which takes into account $\mathcal{B}$'s loss and the optimal value of Eve's loss (by maximizing $\mathcal{E}$'s reconstruction error). This is: 
\begin{equation}
L_{\mathcal{A}\mathcal{B}}(\theta_{\mathcal{A}}, \theta_{\mathcal{B}}) = L_{\mathcal{B}}\left(\theta_{\mathcal{A}}, \theta_{\mathcal{B}}\right) - L_{\mathcal{E}}(\theta_{\mathcal{A}}, O_{\mathcal{E}}\left(\theta_{\mathcal{A}} \right)).
\end{equation}
Note that also this choice may be subject to variations. Optimal configurations for Alice and Bob can hence by found as 
\begin{equation}
(O_{\mathcal{A}}, O_{\mathcal{B}}) = \textrm{argmin}_{(\theta_{\mathcal{A}}, \theta_{\mathcal{B}})}L_{\mathcal{A}\mathcal{B}}(\theta_{\mathcal{A}}, \theta_{\mathcal{B}}) 
\end{equation}
It must be said that such optimal configurations are not necessarily unique. \\
A few words about training. Such procedure is based upon stochastic gradient descent and on estimated values calculated over examples (not on expected values over a distribution). In addition, $O_{\mathcal{E}}$ is not computed for given values of $\theta_{\mathcal{A}}$ , but simply approximated. This is done by alternating the training of Eve with that of Alice and Bob. \\
Roughly speaking, training proceeds follows. At first, $\mathcal{A}$'s outputs may be totally incomprehensible for both $\mathcal{B}$ and $\mathcal{E}$. After a few steps, Bob could discover a way to decrypt Alice's messages at least partially, without Eve being able to do the same. Later, however, $\mathcal{E}$ may start to break this code. Alice and Bob should then find improvements to their security, in a way such that Eve would find impossible to adjust to those ciphers. Note that this kind of alternation is typical in game theory. 

\subsection{ANC and selective protection} 
Authors of ANC described an experiment to study to test this cryptosystem over selective protection: the question of whether neural networks can learn what information to protect. For example, a plaintext may be made up of several components, of which only few of them are required to be kept secret to the adversary. In this case, selective protection would mean a maximization in utility. \\
Consider a dataset consisting of vectors of for values, namely $(A, B, C, D)$. The target is to build and train a system that, given as inputs the first three values, outputs two predictions of $D$:
$\hat{D}$ and $D_{\text{public}}$. The former is most accurate possible estimate, while the latter is defined as the best possible estimate of $D$ that does not reveal any information about the value of $C$. \\
As before, Alice and Bob share a key and both $\mathcal{B}$ and Eve have access to $\mathcal{A}$'s outputs, that are $D_{\text{public}}$ and a ciphertext $T$. Her input is $(A, B, C)$. Bob produces $\hat{D}$, while Eve tries to recover $C$. 
Using strongly correlated values for $(A, B, C)$ and $D$, the authors were able to show that the adversarial training permits approximating $D$ without revealing $C$.

\section{Improvements: CPA-ANC}
This section is dedicated to an extension of Abadi and Andersen's model. In fact, also this last model has some limitations, that are shown in \cite{brazilians}. Authors of this paper elaborated an extension to ANC, that is discusses in the following.

\subsection{Chosen-Plaintext Attack ANC}
\label{cpaanc}
Main limitation to the security of original ANC model lies in Eve's job, that appears to be too hard. It must decrypt a random message having access only to the ciphertext. The consequence is that, under this methodology, Alice and Bob learn to protect against a weak adversary. It is however possible to strengthen Eve by letting it mount a CPA.  \\
Eve chooses two plaintext messages, namely $P_0$ and $P_1$, and sends them to Alice. $\mathcal{A}$ chooses one of the messages randomly, encrypts it to $C$ and sends it to $\mathcal{B}$ and $\mathcal{E}$. The former decrypts the message using a NN, while the latter only outputs $0$ if it believes $P_0$ was encrypted or $1$ if it believes $P_1$ was encrypted.

\subsection{Neural Network Architecture}
A specific network architecture for this model is proposed, based on the continuous extension of logic operator XOR, built to learn an OTP algorithm. \\
Consider mapping bit $0$ to angle $0$ and bit $1$ to angle $\pi$ (XOR can be seen as the sum of the angles). Generalizing bits to a continuous space, the following defines the mapping of a bit $b$ into an angle: 
\begin{equation}
\label{eq:bit-to-angle}
f(b) = arccos(1 - 2b)
\end{equation}
Then consider its inverse:
\begin{equation}
\label{eq:angle-to-bit}
f^{-1}(a) = \frac{1 - cos(a)}{2}
\end{equation}
The new net, named \textit{CryptoNet}, takes as input $P$ and $K$ and, for each bit, applies \ref{eq:bit-to-angle}. Next step is a weight matrix (write $W$) multiplication, followed by the inverse transformation \ref{eq:angle-to-bit}; output is $C$. Note bits $C_i \in (0,1)$. \textit{CryptoNet} will be denote mathematically as the function
\begin{equation}
C = \zeta_n(W, P, K),
\end{equation}
where we recall $n$ is the size of vectors $P$ and $K$.

\subsection{Method}
Let $E_{\mathcal{A}}(\cdot, K)$ be Alice's encryption function and let $D_{\mathcal{B}}(\cdot, K)$ be Bob's decryption function. In this setup, these maps are defined to be the same \textit{CryptoNet}:
\begin{equation}
E_{\mathcal{A}}(\cdot, K) = D_{\mathcal{B}}(\cdot, K) = \zeta_n(W, \cdot, K).
\end{equation}
This means, such network must be the inverse of itself.
More details on the training procedure can be found in the original paper.
\chapter{Chapter title}

\section{Another section}

Tex of the section with example of theorem

\begin{theorem}\label{th1}
Example of theorem
\end{theorem}
\begin{proof}
proof of the theorem
\end{proof}

to be referenced like Theorem \ref{th1}. 

Same for proposition
\begin{proposition}
Example of proposition
\end{proposition}
 or lemma
\begin{definition}
Example of definition
\end{definition}

\begin{remark}
Example of remark
\end{remark}

\begin{lemma}
Example of lemma
\end{lemma}






\newpage
Example of pseudo code of algorithm

\IncMargin{5mm}
\begin{algorithm}[t]
%\footnotesize
% \SetAlgoLined
\vspace{2mm}
\hspace{-5mm} \KwData{$y_{t_{j}}=(y_{t_{j},1},\ldots,y_{t_{j},m_{t_{j}}})$}
\hspace{-5mm} Set parameters $\alpha=\theta P_{0}$, $\theta>0$, $P_{0}\in M_{1}(\mathbb{Y})$\\[0mm]

 \SetKwBlock{Begin}{Initialise}{end}
 \Begin{
$y\leftarrow\emptyset$, $y^{*}=\emptyset$, $m\leftarrow0$, $M\leftarrow0$, $M\leftarrow\{0\}$, $K_{m}\leftarrow0$, 
$w_{0}\leftarrow1$\\
}

 \SetKwBlock{Begin}{For $j=0,\ldots,J$}{end}
 \Begin{
 \SetKwBlock{Begin}{Title set of instructions 1}{end}
 \Begin{
read data $y_{t_{j}}$\\
$m\leftarrow m+\text{card}(y_{t_{j}})$\\
%compute distinct values $y_{t_{j}}^{*}$ in $y_{t_{j}}$\\
%update total no.~of items $m\leftarrow\text{card}(y)$\\
$y^{*}\leftarrow$ distinct values in $y^{*}\cap y_{t_{j}}$\\
$K_{m}=\text{card}(y^{*})$
}

 \SetKwBlock{Begin}{Title set of instructions 2}{end}
 \Begin{
 
 \SetKwBlock{Begin}{for $M\in M$}{end}
  \Begin{
$n\leftarrow t(y_{t_{j}},M)$\\
$w_{n}\leftarrow
w_{M}\,\mathrm{PU}_{\alpha}(y_{t_{j}}\mid y)
$\\[0mm]
%$\sum_{n\in t(y_{m+1:m+n},M)}\hat w_{n}\Pi_{\alpha+\sum_{i=1}^{K_{m+n}}n_{i}\delta_{y_{i}^{*}}}$\\
%assign weight $\hat w_{n}$ to mixture component $\Pi_{\alpha+\sum_{i=1}^{K_{m}}n_{i}\delta_{y_{i}^{*}}}$\\[-1.5mm]
   }
    $M\leftarrow t(y_{t_{j}},M)$\\
 \SetKwBlock{Begin}{for $M\in M$}{end}
  \Begin{       $w_{M}\leftarrow w_{M}/\sum_{\ell \in M}w_{\ell}$
  }
    $X_{t_{j}}\mid y,y_{t_{j}}\sim    \sum_{M\in M}w_{M}\Pi_{\alpha+\sum_{i=1}^{K_{m}}m_{i}\delta_{y_{i}^{*}}}$
%$w_{M}\leftarrow0$ for $M\notin M$
 }
 
\textbf{Return} $y\leftarrow y\cup y_{t_{j}}$
   }
 \caption{Algorithm title}
 \label{algorithm}
\end{algorithm}

which is referred as Algorithm \ref{algorithm}. 



%\selectlanguage{italian}




\newpage
Items in the bibliography to be referenced like this \cite{EK86} and this \cite{EK81}, check the different style for books and articles. 

Abbreviations of Journal names can be found at this link

msc2010.org/MSC2010-CD/extras/serials.pdf




\begin{thebibliography}{99}

\bibitem[Ethier and Kurtz(1981)]{EK81}  {\sc Ethier, S.N.} and {\sc Kurtz, T.G.} (1981). The infinitely-many-neutral-alleles diffusion model. {\em Adv. Appl. Probab.} {\bf 13}, 429--452.

\bibitem[Ethier and Kurtz(1986)]{EK86}  {\sc Ethier, S.N.} and {\sc Kurtz, T.G.} (1986). \emph{Markov processes: characterization and convergence}. Wiley, New York.

\bibitem[Graupe(2007)]{graupe} {\sc Graupe, D.} (2007). \textit{Principles of Artificial Neural Networks (2nd Edition)}. Word Scientific Publishing, Singapore.
 
\bibitem[Anthony(2001)]{anthony} {\sc Anthony, M. } (2001). \textit{Discrete Mathematics of Neural Networks}. Society for Industrial and Applied Mathematics, Philadelphia.

\bibitem[McCulloch and Pitts(1943)]{mcculloch} {\sc McCulloch, W.} and {\sc Pitts, W}. (1943). \textit{A logical calculus of the ideas immanent in nervous activity}. Bulletin of Mathematical Biophysics 5, 115-133.

\bibitem[Hebb(1949)]{hebb} {\sc Hebb, D. O.} (1949). \textit{The organization of behavior; a neuropsychological theory}. Wiley, New York.

\bibitem[Rosenblatt(1958)]{rosenblatt} {\sc Rosenblatt, F.} (1958) \textit{The perceptron: A probabilistic model for information storage and organization in the brain}. Psychological Review, 65.

\bibitem[Rumelhart, Hinton and Williams(1986)]{back} {\sc Rumelhart, D.}, {\sc Hinton, G.} and {\sc Williams, R.} (1986). \textit{Learning representations by back-propagating errors}. Nature 323, 533–536.

\bibitem[Sejnowski and Rosenberg(1987)]{sejnowski} {\sc Sejnowski, T. J.} and {\sc Rosenberg, C. R.} (1987). \textit{Parallel Networks that Learn to Pronounce English Text}. Complex Systems, 1.

\bibitem[Mitchell(1998)]{mit} {\sc Mitchell, M.} (1998). \textit{An introduction to Genetic Algorithms}. MIT Press.

\bibitem[Holland(1975)]{holland} {\sc Holland, J.} (1975). \textit{Adaptation in Natural and Artificial Systems}. MIT Press. 

\bibitem[Volná(2000)]{volna} {\sc Volnà, E.} (2000). \textit{Using Neural Network in Cryptography}. University of Ostrava.

\bibitem[Volná(1998)]{volna2} {\sc Volnà, E.} (1998).\textit{Learning algorithm which learns both architectures and weights of feedforward neural networks.} Neural Network World. Int. Journal on Neural and Mass-Parallel Compo and Inf. Systems.

\bibitem[Kanter, Kinzel and Kanter(2001)]{kanter} {\sc Kanter, I.}, {\sc Kinzel, W.} and {\sc Kanter, E.} (2001). \textit{Secure exchange of information by synchronization of neural networks}. Bar Ilan University.

\bibitem[Klimov, Mityagin and Shamir(2002)]{shamir} {\sc Klimov, A.}, {\sc Mityagin, A.} and {\sc Shamir, A.} (2002). \textit{Analysis of Neural Cryptography}. Weizmann Institute.


\bibitem[Abadi and Andersen(2016)]{google} {\sc Abadi, M.} and {\sc Andersen, D. G.} (2016). \textit{Learning to protect communications with Adversarial Neural Cryptography}. Google Brain.

\bibitem[Coutinho et al.(2018)]{brazilians} {\sc Coutinho, M.}, {\sc Robson de Oliveira Albuquerque, R.}, {\sc Borges, F. }, {\sc Villalba, L. J. G.} and  {\sc Kim T. H.} (2018). \textit{Learning Perfectly Secure Cryptography to Protect Communications with Adversarial Neural Cryptography}. University of Brasília.

\bibitem[Jayachandiran(2018)]{jay} {\sc Jayachandiran, K.} (2018). \textit{A Machine Learning Approach for Cryptanalysis}. Rochester Institute of Technology.

\bibitem[Alani(2012)]{alani} {\sc Alani, M. M.} (2012). \textit{Neuro-cryptanalysis of DES}. World Congress on Internet Security (WorldCIS-2012)

\bibitem[Gohr(2019)]{gohr} {\sc Gohr, A.} (2019). \textit{Improving Attacks on Round-Reduced Speck32/64 Using Deep Learning}. Bundesamt für Sicherheit in der Informationstechnik (BSI).

\bibitem[Nielsen and Chuang(2000)]{nielsen} {\sc Nielsen, M. A.} and {\sc Chuang, I. L.} (2000). \textit{Quantum Computation and Quantum Information}. Cambridge University Press.

\bibitem[Bernstein, Buchmann and Dahmen(2009)]{pqc} {\sc Bernstein, D. J.}, {\sc Buchmann, J.} and {\sc Dahmen, E.} (2009). \textit{Post-Quantum Cryptography}. Springer.

\bibitem[Shi et al.(2020)]{china} {\sc Shi, J.}, {\sc Chen, S.}, {\sc Lu, Y.}, {\sc Feng, Y.}, {\sc Shi, R.}, {\sc Yang, Y.} and {\sc Li, J.} (2020). \textit{An Approach to Cryptography Based on Continuous-Variable Quantum Neural Network}. Nature.

\end{thebibliography}




\end{document}



